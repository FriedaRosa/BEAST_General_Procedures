---
title: "MachineLearning Models"
author: "Frieda"
format: html
editor: visual
---

## Libraries

```{r, message = F}

## Machine Learning models ------------------- #
library(randomForest)
library(randomForestExplainer) # plotting
library(pdp)
library(dplyr)
library(vip)
library(ggplot2)

```

## Variables

```{r}
rm(list=ls())

source_Git <- c("c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/GitHub/BEAST_General_Procedures/Project_Frieda/StaticPredictors/")

# folder path to output folder
out_path <- c(paste0(source_Git, "out/"))

```

## Create model dataframes

```{r}
df <- readRDS(paste0(out_path, "rds/All_predictors.rds")) %>% 
  unique() %>% 
  select(-sp_atlas.adapted1, -avgEffort)%>% unique() %>%
  mutate(HWI = as.numeric(as.character(HWI)),
         Mass = as.numeric(as.character(Mass)),
         Range.Size = as.numeric(as.character(Range.Size)),
         FP = as.numeric(FP),
         tp = as.integer(tp),
         dataset = as.factor(dataset),
         Habitat = as.factor(Habitat),
         RL_Category = as.factor(RL_Category),
         Habitat.Density = as.factor(Habitat.Density),
         Migration = as.factor(Migration),
         Trophic.Level = as.factor(Trophic.Level),
         Trophic.Niche = as.factor(Trophic.Niche),
         Primary.Lifestyle = as.factor(Primary.Lifestyle),
         grain = as.integer(grain)) %>%
  select(-Total_Ncells_samp, -atlas_bearing, -atlas_bearingMinRect, -atlas_circ, -atlas_elonMinRect, -atlas_widthMinRect, -atlas_lengthMinRect, -atlas_xmin, -atlas_ymin, -atlas_xmax, -atlas_ymax, -atlas_yhalf, -atlas_xhalf, -bearing, -bearingMinRect, -circ, -elonMinRect, -widthMinRect, -lengthMinRect, -increment2)

rownames(df) <- NULL
str(df)

highly_correlated <- caret::findCorrelation(cor(df %>% 
                                                  select(-log_R2_1, -Telfer_1_2, -Family, -Order, -verbatim_name) %>% 
                                                  select_if(is.numeric), use="pairwise.complete.obs"), 
                                            cutoff = 0.8, names = T)

df_red_clean <- df %>% select(-all_of(highly_correlated)) %>% distinct(tp, dataset, verbatim_name, .keep_all = T)

```

# Full Model Log Ratio (no splits)

```{r}
model1 <- randomForest(log_R2_1~., data = df_red_clean %>% select(-Telfer_1_2,  -Family, -Order, -verbatim_name),
                         ntree=150, importance=TRUE, na.action = "na.omit")

model1 # 43.22% variance explained
plot(model1)
model1
varImpPlot(model1)
plot_multi_way_importance(measure_importance(model1))
plot_min_depth_distribution(min_depth_distribution(model1))
# explain_forest(model1, interactions = T, path = paste0(out_path, "full_model_logR"))

```

# Full Model Telfer (no splits)

```{r}
model2 <- randomForest(Telfer_1_2~., data = df_red_clean %>% select(-log_R2_1, -Family, -Order, -verbatim_name),
                         ntree=150, importance=TRUE, na.action = "na.omit")

model2 #49.56% Variance explained
plot(model2)
model2
varImpPlot(model2)
plot_multi_way_importance(measure_importance(model2))
plot_min_depth_distribution(min_depth_distribution(model2))
# explain_forest(model2, interactions = T, path = paste0(out_path, "full_model_telfer"))

```

## Create list of data frame for model loop

```{r}
cols <- names(df_red_clean)
cols_logR <- cols[-which(cols == "Telfer_1_2")]
cols_telfer <- cols[-which(cols == "log_R2_1")]

# Log Ratio df
df_log_R <-  df_red_clean %>% 
  select(all_of(cols_logR)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(log_R2_1))

df_log_R1 <- df_log_R %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_log_R2 <- df_log_R %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change

# Telfer df
df_telfer <- df_red_clean %>% 
  select(all_of(cols_telfer)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(Telfer_1_2))

df_telfer1 <- df_telfer %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_telfer2 <- df_telfer %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change


dfs_list <- list(df_log_R1, df_log_R2, df_telfer1, df_telfer2)
names(dfs_list) <- c("df_log_R1", "df_log_R2", "df_telfer1", "df_telfer2)")

```

```{r}
models_list <- list()
models_res_list <-list()
pred_list <- list()
for (dd in seq_along(dfs_list)){
  model_df <- dfs_list[[dd]]
  
  # split into test and train #
  set.seed(123)
  samp <- sample(nrow(model_df), 0.8 * nrow(model_df))
  train <- model_df[samp, ]; dim(train) 
  test <- model_df[-samp, ]; dim(test) 
  
    # create response variable
  response_var <- if ("log_R2_1" %in% names(train)) "log_R2_1" else if ("Telfer_1_2" %in% names(train)) "Telfer_1_2" 

  # build models # 
    
  ## mtry = number of predictors sampled for spliting at each node. (higher values give higher R2?)
  ## ntree = number of trees grown
  ## importance = first column is the mean decrease in accuracy and the second the mean decrease in MSE
  model <- randomForest(as.formula(paste(response_var, "~ .")), data = train, nperm=5,
                         ntree=150, importance=TRUE, na.action = "na.omit")
  data_pred <- data.frame(Predicted = predict(model, newdata = test), Observed = test[[response_var]])
 
  models_list[[dd]] <- model    
  pred_list[[dd]] <- data_pred


  n_vars <- length(model$forest$xlevels)
  min_depth <- min_depth_distribution(model)
  impt_frame <- measure_importance(model)
  
  # explain_forest(model, interactions = T, path = paste0(out_path))
  
  
  pdf(file = paste0(out_path, "Fig_", dd, ".pdf"), paper="a4", onefile = T)
    # Some Figures
    print(plot_min_depth_distribution(min_depth, k = n_vars))
    print(varImpPlot(model, n.var = n_vars, type=1))
    print(plot_multi_way_importance(impt_frame, size_measure = "no_of_nodes"))

    print(ggplot(data_pred, aes(x = Predicted, y = Observed)) +
        geom_point() +
        geom_abline(intercept = 0,
                    slope = 1,
                    color = "red",
                    lwd = 0.7))
  dev.off()
  
  # extract model results # 
  imp <- as.data.frame(model$importance[,1])
  names(imp) <- c("Importance")
  imp$var <- rownames(imp)
  imp2 <- imp %>% tidyr::pivot_wider(names_from = var,
                      values_from = Importance)

  model_res <- data.frame(
    df = names(dfs_list)[[dd]],
    mean_pseudo_R2 = mean(model$rsq),
    mean_mse = mean(model$mse),
    imp2)
  models_res_list[[dd]] <- model_res
  
  
}

names(models_list) <- names(dfs_list)
names(models_res_list) <- names(dfs_list)
names(pred_list) <- names(dfs_list)

saveRDS(models_list, paste0(out_path, "models_list.rds"))
saveRDS(models_res_list, paste0(out_path, "models_res_list.rds"))
saveRDS(pred_list, paste0(out_path, "pred_list.rds"))

plyr::rbind.fill(models_res_list) %>% mutate_if(is.numeric, round,2)

```

## 

## Random Forest models

-   Explanation minimum depth =\
    In a **random forest model**, the **minimum depth** refers to the minimum number of nodes that a decision tree must have before it is considered for splitting. [This parameter is used to control the complexity of the decision trees in the forest and prevent overfitting ^1^](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).

-   The `randomForestSRC` package in R provides a variable selection approach based on a tree-based concept called **minimal depth**. [This approach captures the essence of variable importance measures, but because it involves no randomization, and is simpler to calculate, it can be used as a theoretical basis for variable selection and for speedier calculations for large data ^2^](https://www.randomforestsrc.org/articles/minidep.html).

```{r, randomForest visualizations}

min_depth <- lapply(models_list, min_depth_distribution)
plot_min_depth_distribution(min_depth[[1]])

# Variable importance =====
round(models_list[[1]]$importance,3)
varImpPlot(models_list[[1]])

impt_frame <- measure_importance(models_list[[1]])
plot_multi_way_importance(impt_frame)

ggplot(pred_list[[1]], aes(x = Predicted, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)

```

https://www.css.cornell.edu/faculty/dgr2/\_static/files/R_html/CompareRandomForestPackages.html

```{r, ranger package}
# ----------------------------------- #
train <- train %>% na.omit(); dim(train)
test <- test %>% na.omit(); dim(test)

# Model 2 using ranger package ====
#install.packages("ranger")
library(ranger)

rf2 <- ranger(log_R2_1 ~ ., data=train, importance = "permutation", tuneGrid = data.frame(mtry = ceiling((ncol(test)-1)/3), splitrule = "variance", min.node.size = 5))
pred <- predict(rf2, data = test)
# table(test$log_R2_1, pred$predictions)

rf2
rf2$variable.importance
data_mod2 <- data.frame(prediction = predict(rf2, data = test),  # Create data for ggplot2
                       Observed = test$log_R2_1)
library(ggplot2)
ggplot(data_mod2, aes(x = prediction, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)


```
