---
title: "MachineLearning Models"
author: "Frieda"
format: html
editor: visual
---

## Libraries

```{r, message = F}

## Machine Learning models ------------------- #
library(randomForest)
library(randomForestExplainer) # plotting
library(pdp)
library(dplyr)
library(vip)
library(ggplot2)

```

## Variables

```{r}
rm(list=ls())

source_Git <- c("c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/GitHub/BEAST_General_Procedures/Project_Frieda/StaticPredictors/")

# folder path to output folder
out_path <- c(paste0(source_Git, "out/"))

```

## Create model dataframes

```{r}
df <- readRDS("out/All_predictors.rds") %>% 
  select(-p_value, -neighbor_method) %>% unique() %>% 
  mutate_at(
    c("dataset", "years", "tp", "grain","Habitat", "Migration", "Trophic.Level", "Trophic.Niche", "Primary.Lifestyle", "Habitat.Density"), as.factor) %>% 
  rename("HWI" = "Hand-Wing.Index") 

rownames(df) <- NULL




highly_correlated <- findCorrelation(cor(df %>% na.omit() %>% select_if(is.numeric) %>% select(-atlas_xhalf, -atlas_yhalf), use="pairwise.complete.obs"), cutoff = 0.9, names = T)


df_red_clean <- df %>% na.omit() %>% select_if(is.numeric) %>% select(-atlas_xhalf, -atlas_yhalf)
df_red_clean <- df_red_clean %>% select(-all_of(highly_correlated))


# quick correlation double check:
# cor_df <- df %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% as.data.frame()
# df %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% 
#   ggcorrplot::ggcorrplot(insig="blank")
# #write.csv(cor_df, paste0(out_path, "csv/correlation.csv"))


# Main analysis: Log Ratio of AOO change

cols_all <-  c("tp", "dataset", "verbatim_name", "log_R2_1", # General ID cols
               # Geometry
               "relative_occupancy_Ncells", "D_AOO_a",
               "nsDist",
               "circNorm", "relCirc", "minDist_toBorder", 
              "sp_COG_long", "sp_COG_lat", "Southernness", "Westernness", "Dist_toCOD", 
              "atlas_relCirc",
               "Total_area", "CenterOfGravity_Atlas_lat", "grain",
               # Traits
               "global_moran", "HWI", "FP", "Genus.name", "Family", "Order", "sd_PC1", "sd_PC2",
               "Mass", "Migration", "Trophic.Level", "Trophic.Niche", "Habitat.Density", "Range.Size", "Primary.Lifestyle",
               # Diversity
               "AlphaSR_sp", "BetaSR_sp", "GammaSR", "mean_prob_cooccur", "Dist_toCOD")
 df %>% select(all_of(cols_all)) %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% 
  ggcorrplot::ggcorrplot(insig="blank", hc.order=T, lab=T, lab_size = 2, pch.cex = 1,p.mat = ggcorrplot::cor_pmat(.))              

# Geometry
cols_H1 <- c("tp", "dataset", "verbatim_name", "log_R2_1", "relative_occupancy_Ncells", "D_AOO_s", 
             "relative_occupancy_Ncells", "D_AOO_a",
               "nsDist",
               "circNorm", "relCirc", "minDist_toBorder", 
              "sp_COG_long", "sp_COG_lat", "Southernness", "Westernness", "Dist_toCOD", 
              "atlas_relCirc",
               "Total_area", "CenterOfGravity_Atlas_lat", "grain")

df %>% select(all_of(cols_H1)) %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% 
  ggcorrplot::ggcorrplot(insig="blank", hc.order=T, lab=T, lab_size = 2, pch.cex = 1,p.mat = ggcorrplot::cor_pmat(.))


# Traits
cols_H2 <- c("tp", "dataset", "verbatim_name", "log_R2_1",
             "global_moran", "HWI", "FP", "Genus.name", "Family", "Order", "sd_PC1", "sd_PC2", 
              "Mass", "Migration", "Trophic.Level", "Trophic.Niche", "Habitat.Density", "Range.Size", "Primary.Lifestyle")

df %>% select(all_of(cols_H2)) %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% 
  ggcorrplot::ggcorrplot(insig="blank", hc.order=T, lab=T, lab_size = 2, pch.cex = 1,p.mat = ggcorrplot::cor_pmat(.))


# Diversity
cols_H3 <- c("tp", "dataset", "verbatim_name", "log_R2_1", 
             "AlphaSR_sp", "BetaSR_sp", "GammaSR", "mean_prob_cooccur", "Dist_toCOD")

df %>% select(all_of(cols_H3)) %>% select_if(is.numeric) %>% na.omit() %>% cor() %>% round(2) %>% 
  ggcorrplot::ggcorrplot(insig="blank", hc.order=T, lab=T, lab_size = 2, pch.cex = 1,p.mat = ggcorrplot::cor_pmat(.))


# Sensitivity
cols_Telfer <- c("tp", "dataset", "verbatim_name", "Telfer_1_2", # General ID cols
               # Geometry
               "relative_occupancy_Ncells", "D_AOO_a",
               "nsDist",
               "circNorm", "relCirc", "minDist_toBorder", 
              "sp_COG_long", "sp_COG_lat", "Southernness", "Westernness", "Dist_toCOD", 
              "atlas_relCirc",
               "Total_area", "CenterOfGravity_Atlas_lat", "grain",
               # Traits
               "global_moran", "HWI", "FP", "Genus.name", "Family", "Order", "sd_PC1", "sd_PC2",
               "Mass", "Migration", "Trophic.Level", "Trophic.Niche", "Habitat.Density", "Range.Size", "Primary.Lifestyle",
               # Diversity
               "AlphaSR_sp", "BetaSR_sp", "GammaSR", "mean_prob_cooccur", "Dist_toCOD")



# Full Data #
# (42 predictors)
df_red <- df %>% 
  select(all_of(cols_all)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(log_R2_1))

df_red1 <- df_red %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_red2 <- df_red %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change


#Telfer
df_telfer <- df %>% 
  select(all_of(cols_Telfer)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(Telfer_1_2))

df_telfer1 <- df_telfer %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_telfer2 <- df_telfer %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change

# H1: Geometry
H1_Geometry_df <- df_red %>% select(all_of(cols_H1)) %>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H1_Geometry_df1 <- H1_Geometry_df %>% filter(tp==1) %>% select(-tp, -verbatim_name)     
H1_Geometry_df2 <- H1_Geometry_df %>% filter(tp==2) %>% select(-tp, -verbatim_name)     
  



# H2: Traits
H2_Traits_df <- df_red %>% select(all_of(cols_H2))%>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H2_Traits_df1 <-  H2_Traits_df %>% filter(tp==1) %>% select(-tp, -verbatim_name) 
H2_Traits_df2 <-  H2_Traits_df %>% filter(tp==2) %>% select(-tp, -verbatim_name) 




# H3 Diversity:
H3_Diversity_df <- df_red %>% select(all_of(cols_H3))%>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H3_Diversity_df1 <- H3_Diversity_df  %>% filter(tp==1) %>% select(-tp, -verbatim_name)  
H3_Diversity_df2 <- H3_Diversity_df  %>% filter(tp==2) %>% select(-tp, -verbatim_name)  




# save in list #
dfs_list <- list(df_red1, df_telfer1, H1_Geometry_df1, H2_Traits_df1, H3_Diversity_df1, df_red2, df_telfer2, H1_Geometry_df2, H2_Traits_df2, H3_Diversity_df2)
names(dfs_list) <- c("df_red1", "df_telfer1","H1_Geometry_df1", "H2_Traits_df1", "H3_Diversity_df1", "df_red2", "df_telfer2","H1_Geometry_df2", "H2_Traits_df2", "H3_Diversity_df2")


# clear environment #
rm(df_red1, H1_Geometry_df1, H2_Traits_df1, H3_Diversity_df1, df_red2, H1_Geometry_df2, H2_Traits_df2, H3_Diversity_df2, df_telfer1, df_telfer2)

```

```{r}


models_list <- list()
models_res_list <-list()
pred_list <- list()


for (dd in seq_along(dfs_list)){

  
  model_df <- dfs_list[[dd]]
  
  # split into test and train #
  set.seed(123)
  samp <- sample(nrow(model_df), 0.8 * nrow(model_df))
  train <- model_df[samp, ]; dim(train) 
  test <- model_df[-samp, ]; dim(test) 
  
    # create response variable
  response_var <- if ("log_R2_1" %in% names(train)) "log_R2_1" else if ("Telfer_1_2" %in% names(train)) "Telfer_1_2" 

  # build models # 
    
  ## mtry = number of predictors sampled for spliting at each node. (higher values give higher R2?)
  ## ntree = number of trees grown
  ## importance = first column is the mean decrease in accuracy and the second the mean decrease in MSE
  model <- randomForest(as.formula(paste(response_var, "~ .")), data = train, nperm=5,
                         ntree=150, importance=TRUE, na.action = "na.omit")
  data_pred <- data.frame(Predicted = predict(model, newdata = test), Observed = test[[response_var]])
 
  models_list[[dd]] <- model    
  pred_list[[dd]] <- data_pred


  n_vars <- length(model$forest$xlevels)
  min_depth <- min_depth_distribution(model)
  impt_frame <- measure_importance(model)
  #explain_forest(model, interactions = T, path = paste0(out_path))
  
  
  pdf(file = paste0(out_path, "Fig_", dd, ".pdf"), paper="a4", onefile = T)
    # Some Figures
    print(plot_min_depth_distribution(min_depth, k = n_vars))
    print(varImpPlot(model, n.var = n_vars, type=1))
    print(plot_multi_way_importance(impt_frame, size_measure = "no_of_nodes"))

    print(ggplot(data_pred, aes(x = Predicted, y = Observed)) +
        geom_point() +
        geom_abline(intercept = 0,
                    slope = 1,
                    color = "red",
                    lwd = 0.7))
  dev.off()
  
  # extract model results # 
  imp <- as.data.frame(model$importance[,1])
  names(imp) <- c("Importance")
  imp$var <- rownames(imp)
  imp2 <- imp %>% tidyr::pivot_wider(names_from = var,
                      values_from = Importance)

  model_res <- data.frame(
    df = names(dfs_list)[dd],
    mean_pseudo_R2 = mean(model$rsq),
    mean_mse = mean(model$mse),
    imp2)
  models_res_list[[dd]] <- model_res
  
  
}

names(models_list) <- names(dfs_list)
names(models_res_list) <- names(dfs_list)
names(pred_list) <- names(dfs_list)

saveRDS(models_list, paste0(out_path, "models_list.rds"))
saveRDS(models_res_list, paste0(out_path, "models_res_list.rds"))
saveRDS(pred_list, paste0(out_path, "pred_list.rds"))

plyr::rbind.fill(models_res_list) %>% mutate_if(is.numeric, round,2)

```

## 

## Random Forest models

-   Explanation minimum depth =\
    In a **random forest model**, the **minimum depth** refers to the minimum number of nodes that a decision tree must have before it is considered for splitting. [This parameter is used to control the complexity of the decision trees in the forest and prevent overfitting ^1^](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).

-   The `randomForestSRC` package in R provides a variable selection approach based on a tree-based concept called **minimal depth**. [This approach captures the essence of variable importance measures, but because it involves no randomization, and is simpler to calculate, it can be used as a theoretical basis for variable selection and for speedier calculations for large data ^2^](https://www.randomforestsrc.org/articles/minidep.html).

```{r, randomForest visualizations}

min_depth <- lapply(models_list, min_depth_distribution)
plot_min_depth_distribution(min_depth[[1]])

# Variable importance =====
round(models_list[[1]]$importance,3)
varImpPlot(models_list[[1]])

impt_frame <- measure_importance(models_list[[1]])
plot_multi_way_importance(impt_frame)

ggplot(pred_list[[1]], aes(x = Predicted, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)

```

https://www.css.cornell.edu/faculty/dgr2/\_static/files/R_html/CompareRandomForestPackages.html

```{r, ranger package}
# ----------------------------------- #
train <- train %>% na.omit(); dim(train)
test <- test %>% na.omit(); dim(test)

# Model 2 using ranger package ====
#install.packages("ranger")
library(ranger)

rf2 <- ranger(log_R2_1 ~ ., data=train, importance = "permutation", tuneGrid = data.frame(mtry = ceiling((ncol(test)-1)/3), splitrule = "variance", min.node.size = 5))
pred <- predict(rf2, data = test)
# table(test$log_R2_1, pred$predictions)

rf2
rf2$variable.importance
data_mod2 <- data.frame(prediction = predict(rf2, data = test),  # Create data for ggplot2
                       Observed = test$log_R2_1)
library(ggplot2)
ggplot(data_mod2, aes(x = prediction, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)


```


