---
title: "MachineLearning Models"
author: "Frieda"
format: html
editor: visual
---

```{r, message = F}
# install.packages("drat", repos="https://cran.rstudio.com")
# drat:::addRepo("dmlc")
# install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")

## Machine Learning models ------------------- #
library(randomForest)
library(randomForestExplainer) # plotting
library(ranger)
library(gbm)
library(xgboost)
library(caret)
library(caretEnsemble)
library(pdp)


library(dplyr)
```

```{r}
rm(list=ls())

source_Git <- c("c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/GitHub/BEAST_General_Procedures/Project_Frieda/StaticPredictors/")

# folder path to output folder
out_path <- c(paste0(source_Git, "out/"))

df <- readRDS("out/All_predictors.rds") %>% select(-p_value) %>% unique() %>% select(-neighbor_method) %>%   
  mutate_at(
    c("dataset", "years", "tp", "grain","Habitat", "Migration", "Trophic.Level", "Trophic.Niche", "Primary.Lifestyle", "Habitat.Density"), as.factor) %>% rename("HWI" = "Hand-Wing.Index") 
rownames(df) <- NULL



names(df)

df_red <- df %>% select(tp, dataset, verbatim_name, log_R2_1,
                                relative_occupancy_Ncells, D_AOO_s,
                                nsDist, ewDist, maxDist, lengthMinRect, widthMinRect, elonMinRect, elonRatio, 
                                circ, circNorm, relCirc, lin, bearingMinRect, bearing, Dist_toCOD, minDist_toBorder, 
                                maxDist_toBorder, sp_COG_long, sp_COG_long, Southernness, Westernness,
                                atlas_nsDist, atlas_ewDist, atlas_maxDist, atlas_lengthMinRect, atlas_widthMinRect, atlas_elonMinRect,
                                atlas_elonRatio, atlas_circ, atlas_circNorm, atlas_relCirc, atlas_lin, atlas_bearingMinRect, atlas_bearing,
                                Total_area, CenterOfGravity_Atlas_long, CenterOfGravity_Atlas_lat, grain, 
                                global_moran, HWI, FP, Genus.name, Family, Order, sd_PC1, sd_PC2, var_PC1, var_PC2, 
                                Mass, Migration, Trophic.Level, Trophic.Niche, Habitat.Density, Range.Size, Primary.Lifestyle, 
                                AlphaSR_sp, BetaSR_sp, GammaSR, mean_prob_cooccur)%>% 
                distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
                filter(!is.na(log_R2_1))
pred <- df_red %>% select(-log_R2_1)
resp <- df_red %>% select(log_R2_1)
#imputed_df <- rfImpute(x = pred, y = resp, iter = 5, ntree = 50000)


df_red1 <- df_red %>% filter(tp == 1) %>% select(-tp, -verbatim_name) 
df_red2 <- df_red %>% filter(tp == 2) %>% select(-tp, -verbatim_name) 


H1_Geometry_df <- df_red %>% select(tp, dataset, verbatim_name, log_R2_1,
                                relative_occupancy_Ncells,
                                nsDist, ewDist, maxDist, lengthMinRect, widthMinRect, elonMinRect, elonRatio, 
                                circ, circNorm, relCirc, lin, bearingMinRect, bearing, minDist_toBorder, 
                                maxDist_toBorder, sp_COG_long, sp_COG_long, Southernness, Westernness,
                                atlas_nsDist, atlas_ewDist, atlas_maxDist, atlas_lengthMinRect, atlas_widthMinRect, atlas_elonMinRect,
                                atlas_elonRatio, atlas_circ, atlas_circNorm, atlas_relCirc, atlas_lin, atlas_bearingMinRect, atlas_bearing,
                                Total_area, CenterOfGravity_Atlas_long, CenterOfGravity_Atlas_lat, grain, 
                                global_moran) %>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H1_Geometry_df1 <- H1_Geometry_df %>% filter(tp==1) %>% select(-tp, -verbatim_name)     
H1_Geometry_df2 <- H1_Geometry_df %>% filter(tp==2) %>% select(-tp, -verbatim_name)     
  




H2_Traits_df <- df_red %>% select(tp, dataset, verbatim_name, log_R2_1,
                              HWI, FP, Genus.name, Family, Order, sd_PC1, sd_PC2, var_PC1, var_PC2, 
                                Mass, Migration, Trophic.Level, Trophic.Niche, Habitat.Density, Range.Size, Primary.Lifestyle)%>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H2_Traits_df1 <-  H2_Traits_df %>% filter(tp==1) %>% select(-tp, -verbatim_name) 
H2_Traits_df2 <-  H2_Traits_df %>% filter(tp==2) %>% select(-tp, -verbatim_name) 





H3_Diversity_df <- df_red %>% select(tp, dataset, verbatim_name, log_R2_1,
                                 AlphaSR_sp, BetaSR_sp, GammaSR, mean_prob_cooccur,  Dist_toCOD)%>% 
  distinct(tp, dataset, verbatim_name, .keep_all = T)
H3_Diversity_df1 <- H3_Diversity_df  %>% filter(tp==1) %>% select(-tp, -verbatim_name)  
H3_Diversity_df2 <- H3_Diversity_df  %>% filter(tp==2) %>% select(-tp, -verbatim_name)  

# save in list #
dfs_list <- list(df_red1, H1_Geometry_df1, H2_Traits_df1, H3_Diversity_df1, df_red2, H1_Geometry_df2, H2_Traits_df2, H3_Diversity_df2)
names(dfs_list) <- c("df_red1", "H1_Geometry_df1", "H2_Traits_df1", "H3_Diversity_df1", "df_red2", "H1_Geometry_df2", "H2_Traits_df2", "H3_Diversity_df2")
# clear environment #
rm(df_red1, H1_Geometry_df1, H2_Traits_df1, H3_Diversity_df1, df_red2, H1_Geometry_df2, H2_Traits_df2, H3_Diversity_df2)
```

```{r}


models_list <- list()
models_res_list <-list()
pred_list <- list()


for (df in seq_along(dfs_list)){

  
  model_df <- dfs_list[[df]]
  
  # split into test and train #
  set.seed(123)
  samp <- sample(nrow(model_df), 0.8 * nrow(model_df))
  train <- model_df[samp, ]; dim(train) 
  test <- model_df[-samp, ]; dim(test) 
  
  # build models # 
    
  ## mtry = number of predictors sampled for spliting at each node. (higher values give higher R2?)
  ## ntree = number of trees grown
  ## importance = first column is the mean decrease in accuracy and the second the mean decrease in MSE
  
  model <- randomForest(log_R2_1 ~ ., data = train, ntree=50000, mtry=9, importance=TRUE, na.action = "na.omit")
  models_list[[df]] <- model
  
  # extract model results # 
  imp <- as.data.frame(model$importance[,1])
  names(imp) <- c("Importance")
  imp$var <- rownames(imp)
  imp2 <- imp %>% pivot_wider(names_from = var,
                      values_from = Importance)

  model_res <- data.frame(
    df = names(dfs_list)[[df]],
    mean_pseudo_R2 = mean(model$rsq),
    mean_mse = mean(model$mse),
    imp2)
  models_res_list[[df]] <- model_res
  
  
  # Prediction #
  data_pred <- data.frame(Predicted = predict(model, newdata = test),
                          Observed = test$log_R2_1)
 
  pred_list[[df]] <- data_pred
  
}


saveRDS(models_list, paste0(out_path, "models_list.rds"))
saveRDS(models_res_list, paste0(out_path, "models_res_list.rds"))
saveRDS(pred_list, paste0(out_path, "pred_list.rds"))


```

## 

## Random Forest models

-   Explanation minimum depth =\
    In a **random forest model**, the **minimum depth** refers to the minimum number of nodes that a decision tree must have before it is considered for splitting. [This parameter is used to control the complexity of the decision trees in the forest and prevent overfitting ^1^](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).

-   The `randomForestSRC` package in R provides a variable selection approach based on a tree-based concept called **minimal depth**. [This approach captures the essence of variable importance measures, but because it involves no randomization, and is simpler to calculate, it can be used as a theoretical basis for variable selection and for speedier calculations for large data ^2^](https://www.randomforestsrc.org/articles/minidep.html).

```{r, make model dfs}



# Create model dataframe --------------------------------------------------------------------- #
model_df <- df %>%
  ungroup() %>%
  distinct(.)

model_df1 <- model_df  %>% filter(tp == 1) %>% select(-Telfer_1_2, -tp, -verbatim_name) # Predicting Future Change
model_df1_past <- model_df %>% filter(tp == 2)  %>% select(-Telfer_1_2, -tp) # Predicting Past Change

model_df2 <- model_df %>% select(-log_R2_1) %>% filter(tp == 1) # Future Change
model_df2_past <- model_df %>% select(-log_R2_1) %>% filter(tp == 2) # Past Change
# --------------------------------------------------------------------------------- #
# Data partitioning =====
set.seed(123)

samp <- sample(nrow(model_df1), 0.8 * nrow(model_df1))
train <- model_df1[samp, ]; dim(train) # 1842 species
test <- model_df1[-samp, ]; dim(test) # 461 species


## Sensitivity: Telfer
samp_ss <- sample(nrow(model_df2), 0.8 * nrow(model_df2))
train_ss <- model_df2[samp_ss,];dim(train_ss)
test_ss <- model_df2[-samp_ss,];dim(test_ss)


```

-   Start: 11:08h, 05.03.24

```{r, randomForest package}
# --------------------------------------------------------------------------------- #
# Model 1 using randomForest package ====

# --------------------------------------------------------------------------------- #
## Control Model with Telfer as Response:
sp.rf_ss <- randomForest(Telfer_1_2 ~ ., data=train_ss, ntree=50000, mtry=8, importance=TRUE, proximity=TRUE, na.action = na.omit) 
sp.rf_ss 

# Call:
#  randomForest(formula = Telfer_1_2 ~ ., data = train_ss, ntree = 50000,      mtry = 8, importance = TRUE, proximity = TRUE, localImp = TRUE,      na.action = na.omit) 
#                Type of random forest: regression
#                      Number of trees: 50000
# No. of variables tried at each split: 8
# 
#           Mean of squared residuals: 0.790031
#                     % Var explained: 22.23

# --------------------------------------------------------------------------------- #
## mtry
## number of observations in the terminal group = number of splits (check default: 5-10) Do not get finer

sp.rf <- randomForest(log_R2_1 ~ ., data=train, ntree=50000, mtry=8, importance=TRUE, proximity=TRUE, na.action = na.omit, localImp = TRUE) 
sp.rf 
saveRDS(sp.rf, paste0(out_path, "randomForest_model.rds"))

# RESULT: OUTPUT:
# Mean of squared residuals: 0.1297556
# % Var explained: 61.21
# saveRDS(sp.rf, paste0(out_path, "randomForest_model.rds"))

min_depth_frame <- min_depth_distribution(sp.rf)
plot_min_depth_distribution(min_depth_frame)

# Variable importance =====
round(sp.rf$importance,3)
varImpPlot(sp.rf)

impt_frame <- measure_importance(sp.rf)
plot_multi_way_importance(impt_frame)

# Prediction ====
prediction <- predict(sp.rf, newdata = test)
# table(prediction, test$log_R2_1)
# prediction
# ---------------------------------------------------------------------------- #
data_mod <- data.frame(Predicted = predict(sp.rf, newdata = test),  # Create data for ggplot2
                       Observed = test$log_R2_1)

ggplot(data_mod, aes(x = Predicted, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)

```

```{r, ranger package}
# ----------------------------------- #
train <- train %>% na.omit(); dim(train)
test <- test %>% na.omit(); dim(test)

# Model 2 using ranger package ====
#install.packages("ranger")
library(ranger)

rf2 <- ranger(log_R2_1 ~ ., data=train, importance = "permutation", tuneGrid = data.frame(mtry = ceiling((ncol(test)-1)/3), splitrule = "variance", min.node.size = 5))
pred <- predict(rf2, data = test)
# table(test$log_R2_1, pred$predictions)

rf2
rf2$variable.importance
data_mod2 <- data.frame(prediction = predict(rf2, data = test),  # Create data for ggplot2
                       Observed = test$log_R2_1)
library(ggplot2)
ggplot(data_mod2, aes(x = prediction, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              lwd = 0.7)


```

##### Different rf model approach

```{r}
library(caret)
library(caretEnsemble)
library(ranger)
library(vip)
library(lmerTest)


names_v <- names(train)
names_v <- names_v[-21]



#traindata <- trainControl(method = "repeatedcv", number = 10, repeats = 3)



train$RS_scaled <- scales::rescale(train$Range.Size, to = c(0, 1))
#train$AOO_scaled <- scales::rescale(train$, to = c(0, 1))
#train$occu_Ncells_scaled <- scales::rescale(train$occupancy_Ncells, to = c(0, 1))
train$mean_area_scaled <- scales::rescale(train$mean_area, to = c(0, 1))
train$mean_area_cropped_scaled <- scales::rescale(train$mean_area_cropped, to = c(0, 1))
#train$mean_effort_scaled <- scales::rescale(train$mean_effort, to = c(0, 1))




lmer_model <- lmerTest::lmer(log_R2_1 ~.+ 
                           (1 | Order3 / Family3), 
                         data = train)

summary(lmer_model)

round(summary(lmer_model)$coefficient,3)


# ------------------------------------------------------------ #

model_list_tp1 <-
  caretList(
    log_R2_1 ~ ., 
    data = train,
    #trControl = traindata,
    metric = "RMSE",
    tuneList = list(
      rf =  caretModelSpec(method = "ranger", 
                           importance = "permutation",
                           tuneGrid = data.frame(mtry = ceiling((ncol(model_df)-1)/3),
                                                                    splitrule = "variance",
                                                                    min.node.size = 5)),
      gbm = caretModelSpec(method = "gbm",
                           distribution = "gaussian",
                           # keep.data = TRUE,
                           tuneGrid = data.frame(n.trees = 500,
                                                 interaction.depth = 1,
                                                 shrinkage = 0.1,
                                                 n.minobsinnode = 5)),
      xgboost = caretModelSpec(method = "xgbTree",
                               tuneGrid = expand.grid(nrounds = 500,
                                                     max_depth = 5,
                                                     eta = 0.1,
                                                     gamma = 0,
                                                     colsample_bytree = 1,
                                                     min_child_weight = 1,
                                                     subsample = 1))
    )
  )


vip(model_list_tp1[[1]], geom = "point", num_features = 25)
vip(model_list_tp1[[3]], geom = "point", num_features = 25)
saveRDS(model_list_tp1, paste0(out_path, "model_list_tp1.rds"))


```

```{r}

library(pdp)


# Figure 2 (right)
model_list_tp1[[1]] %>% # the %>% operator is read as "and then"
partial(pred.var = "m_AOO") %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = "average predicted temporal change")


model_list_tp1[[3]] %>% # the %>% operator is read as "and then"
partial(pred.var = "FP") %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = "average predicted temporal change")

pp1 <- partial(model_list_tp1[[1]], pred.var = c(names_v[1:3]), plot=F)
pp2 <- partial(model_list_tp1[[1]], pred.var = c(names_v[4:6]), plot=F)
pp3 <- partial(model_list_tp1[[1]], pred.var = c(names_v[7:9]), plot=F)
pp4 <- partial(model_list_tp1[[1]], pred.var = c(names_v[10:12]), plot=F)
pp5 <- partial(model_list_tp1[[1]], pred.var = c(names_v[13:15]), plot=F)
pp6 <- partial(model_list_tp1[[1]], pred.var = c(names_v[16:18]), plot=F)
pp7 <- partial(model_list_tp1[[1]], pred.var = c(names_v[19:21]), plot=F)

partialPlotlist <- list(pp1,pp2,pp3,pp4,pp5,pp6,pp7)

```

```{r}
autoplot(model_list_tp1)

save.image(paste0(out_path, "MachineLearning.RData"))
```

# Testing the Hypotheses

## 1. Geometry

```{r}

```
