---
title: "Assignment_Wölke_Stats"
author: "Friederike Wölke"
date: 29.05.2024
format: pdf
toc: TRUE
editor: visual
---

# Assignment for PhD Stats class 2024

## **Topic of Credit Report -- Statistical Methods** - Prague, 07.05.2024

[Title of report:]{.underline} Towards predicting temporal biodiversity change from static patterns

[Name:]{.underline} Friederike Wölke

[Supervisor:]{.underline} Petr Keil

[Thesis title:]{.underline} Universal imprints of temporal change in static spatial patterns of biodiversity

[**Expected methods applied:**]{.underline} Machine learning (random forest regression using the caret suite of tools for hyperparameter tuning), visualization of results.

[Abstract:]{.underline}

The world is undergoing significant environmental transformations, impacting biodiversity and ecosystem functions. Since obtaining temporal replication of biodiversity data is challenging due to cost and monitoring limitations, I aim at predicting temporal trends in species occupancy without requiring temporally replicated data.

Biodiversity kinetics leave characteristics imprint in the spatial patterns that we can see from georeferenced presence/absence data because the underlying processes such as extinction and colonization happen across space.

I aim at predicting the log ratio of temporal change in occupancy (i.e., the sum of area occupied by a species) between two sampling periods from a set of predictor variables that are either related to **H1) species traits and ecology**, **H2)  geometric features of the species range from a single sampling period**, **H3) biodiversity equilibrium dynamics via spatial diversity patterns**,  or **H4) to the characteristics of the study region** -- all of which may equally contribute and act in concert to explaining the temporal process that is underlying the spatial pattern.

For this I use high-quality, spatially continuous atlas data from four breeding bird atlases from temperate zones across the globe. Namely, I assess universal imprints of temporal change in breeding birds in Czech Republic, Japan, New York State and the whole of Europe across two aggregated sampling periods that took place pre-2000 and post-2000. Since data for two sampling periods are available, I will additionally test whether imprints in the spatial aggregation of species can better predict past or future biodiversity change.

For this, I collated 60 predictor variables that vary across sampling periods - each belonging to one of the hypotheses mentioned above. I will use random forest regression to determine the capability of static patterns to predict temporal change, identify the most important predictor variables and compare observed versus predicted results.

If my model can predict temporal change from static patterns, this method will be a useful tool for estimating temporal change in areas and for species where repeated monitoring might not be feasible. If the models are only partially able to predict temporal trends, the important predictors may still yield insights into how temporal processes are acting across space. Additionally disentangling whether imprints of biodiversity kinetics are better at explaining past versus future change may help to understand the temporal dimensions of the imprints.

# Miscellaneous

## Libraries

I will use the caret package for the implemented machine learning models.

```{r}
# Data handling & visualization
library(dplyr)
library(ggplot2)
library(reshape2)


# Machine Learning packages
library(caret)
library(ranger)
library(randomForest)

```

## Read data

Since the raw data is not open, I am providing the (reduced) predictor table that I calculated from it and other external data.

The data has bird species in rows and their predictor data across different datasets in columns. The column 'log_R2_1' is the log ratio of AOO between two time periods (indicated with tp = 1 or tp = 2) and is the response for my temporal change models. 'Telfer_1_2' is another measure of temporal change, though it is relative for each species in a dataset in comparison to the average other species. Telfer will not be further investigated in this assignment.

```{r}
rm(list = ls()) # start with clean environment
gc()

dat <- readRDS('../out/rds/Final_data.rds')
head(dat)
```

# Preparations

## Data handling

Here I'm excluding some predictors which are overlapping with some others.

Next, I'm splitting the datat into their time periods. In the following I will only continue to investigate the change from period 1 to period 2 (i.e, future change).

Some predictor columns have NAs that result from either very rare species or highly cosmopolintain species (thus resulting in division by 0 during computation). WIth knowledge of how I computed the predictors, I manually set some rows with NAs to 0 or 1.

Spatial autocorrelation (Morans I) cannot be calculated for species occupying 100% of an area, thus resulting in NA. these species are removed completly from the model.

```{r}
dat_red <- dat %>% 
  # Response & Predictors: 
  select(log_R2_1, Telfer_1_2, tp, verbatim_name,
         # Atlas specifics: [4:8]
         dataset, mean_area, Total_area_samp, Total_Ncells_samp, mean_cell_length,
         # Diversity metrics [9:11]
         GammaSR, AlphaSR_sp, BetaSR_sp, 
         # Occupancy [12:15]
         AOO, rel_occ_Ncells, D_AOO_a, mean_prob_cooccur,
         # BirdLife data [16:19]
         sd_PC1, sd_PC2, GlobRangeSize_m2, IUCN, 
         # Spatial autocorrelation [20:21]
         moran, x_intercept, 
         # AVONET [22:30]
         Mass, Habitat, Habitat.Density, Migration, Trophic.Level, Trophic.Niche, Primary.Lifestyle, FP, HWI, 
         # Species range centroid [31:32]
         sp_centr_lon, sp_centr_lat,
         # Geometry1: [33:36] 
         lengthMinRect, widthMinRect, elonMinRect, bearingMinRect,
         # Geometry2: [37:40]
         circ, bearing, Southernness, Westernness, 
         # Geometry3: [41:47]
         rel_maxDist, rel_ewDist, rel_nsDist, rel_elonRatio, rel_relCirc, rel_circNorm, rel_lin,
         # Geometry4: [48:51]
         Dist_centroid_to_COG, maxDist_toBorder_border, maxDist_toBorder_centr, minDist_toBorder_centr,
         # Geometry 5: [52:59]
         atlas_lengthMinRect, atlas_widthMinRect, atlas_elonMinRect, atlas_circ, atlas_bearingMinRect, atlas_bearing, AtlasCOG_long, AtlasCOG_lat)

# Columns that were removed ====
setdiff(names(dat), names(dat_red))

# Subset data to different sampling periods ====
dat1 <- dat_red %>% filter(tp == 1) %>% select(-tp)

# manually fixing NAs in some predictors that lead to NAs in the calculation:
# e.g. if a species covers 100% of an area, the fractal dimension is NA but it's actually D = 2 (a filled plane). 
# Similarly, mean probability to co-occur is NA when two species have 0 probability to co-occur because both are extremely rare, thus we set those to = 0.)
# Lastly, I remove species that have Moran's I = NA. It's those species that cover the whole area (spatial autocorr. cannot be calculated)
dat1 <- dat1 %>% 
  mutate(D_AOO_a = 
           case_when(
             is.na(D_AOO_a) & rel_occ_Ncells > 0.97 ~ 2,
                     TRUE ~ D_AOO_a),
         mean_prob_cooccur = 
           case_when(
             is.na(mean_prob_cooccur) & rel_occ_Ncells < 0.05 ~ 0,
                     TRUE ~ mean_prob_cooccur)) %>%
  filter(!is.na(moran))
```

## Raw data visualization

First we'll have a look at the distribution of the predictor variables. Although randomForest models are able to capture complex data, we want to see how much the data deviates from normal distributions and how much of the variation is based on the specific dataset (i.e., Europe, Czech republic, Japan, New York state).

```{r, fig.height = 10, fig.width= 12}
# Visualize data ====
## Species predictors
dat1 %>% 
  select(starts_with("atlas"), starts_with("Atlas"), mean_area, Total_area_samp, Total_Ncells_samp, mean_cell_length, GammaSR, dataset) %>% 
  melt() %>%
  ggplot(aes(x = value, fill = dataset))+
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_x") +
  theme_minimal() + 
  labs(title = "Histograms of All Atlas Columns",
       x = "Value",
       y = "Frequency")

## Atlas predictors
dat1 %>%  
  select(-starts_with("atlas"), -starts_with("Atlas"), -mean_area, -Total_area_samp, -Total_Ncells_samp, -mean_cell_length, -GammaSR,) %>% 
  melt() %>%
  ggplot(aes(x = value, fill = dataset)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Histograms of All Species Columns",
       x = "Value",
       y = "Frequency")


```

Next, let's look at the interactions between variables using featurePlots.

```{r, fig.height = 10, fig.width= 12}
# Feature plots =====
featurePlot(x = dat1[5:8],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[9:11],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[12:15],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[16:21],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[22:30],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[31:32],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[33:36],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[37:40],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[41:47],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[48:51],
            y = dat1$log_R2_1, 
            plot = 'pairs')

featurePlot(x = dat1[52:59],
            y = dat1$log_R2_1, 
            plot = 'pairs')
```

# Machine Learning

I will use a random forest regression model to determine the predictive strength of static predictors for future temporal change.

## Pre-Processing

So far, I have a set of 56 predictor variables to infer the response variable 'log_R2_1'. Most of these are calculated from the spatial data and thus may be correlated. We will remove correlations \> 0.9 using the PreProcess() function for the full dataset. In addition, we will use median imputation to fix any NAs that do not result from extreme data points.

Additionally we will check for variables with low variance, since these inherently will not explain much of the response.

Since random forest models do not estimate coefficients, scaling and centering is not necessary.

### 1. Near Zero Variables & Correlations

We will remove correlations \> 0.9 because these cannot be discerned anyway. This will leave us with 43 predictor variables in total which will go into the primary model & subsequent feature selection and hyperparameter tuning.

```{r}
# Pre-processing ====
## 1. Near Zero Vars
nzv <- nearZeroVar(dat1, saveMetrics = T) 
nzv %>% filter(nzv == T) # only IUCN, but this is an important predictor (!) we will keep it.

## 2. Correlations
pp <- preProcess(dat1[,-c(1:3)], method = c('corr', 'medianImpute')) # exclude both indicators of temporal change & verbatim_name
pp$method
dat1_v2 <- cbind(dat1[,c(1:3)], predict(pp, dat1[,-c(1:3)]))

colSums(is.na(dat1))[colSums(is.na(dat1)) != 0] 
colSums(is.na(dat1_v2))[colSums(is.na(dat1_v2)) != 0] # only categorical variables with NAs left

dat1_v2 <- dat1_v2 %>% na.omit()

```

#### Visualize data again

Since we imputed and removed some data, let's have a look at it again.

```{r, fig.height = 10, fig.width= 12}
## Species predictors
dat1_v2 %>% 
  select(starts_with("atlas"), starts_with("Atlas"), Total_Ncells_samp, GammaSR, dataset) %>% 
  melt() %>%
  ggplot(aes(x = value, fill = dataset))+
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_x") +
  theme_minimal() + 
  labs(title = "Histograms of All Atlas Columns",
       x = "Value",
       y = "Frequency")

## Atlas predictors
dat1_v2 %>%  
  select(-starts_with("atlas"), -starts_with("Atlas"), -Total_Ncells_samp, -GammaSR,) %>% 
  melt() %>%
  ggplot(aes(x = value, fill = dataset)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Histograms of All Species Columns",
       x = "Value",
       y = "Frequency")

```

### Train & Test split

We will start with an initial test/train split for evaluation of the final model. We will use 5 resamples to achieve robust results.

```{r}
set.seed(42)

# Create training (80%) and testing (20%) sets
trainIndex <- createDataPartition(dat1_v2$log_R2_1, list = FALSE, p = 0.8, 5) # 5 resamples
trainData <- dat1_v2[trainIndex[,1], ] # first resample
testData <- dat1_v2[-trainIndex[,1], ] # first resample

```

```{r}
rm(dat, dat_red, dat1, dat2, nzv, pp)
save.image("Assignment_ws_rfFuncs.RData")
```

# Extra: GLM - Coefficients

To get an idea of how predictors influence temporal change, I additionally run a general linear model to get some coefficients. Since many predictors are included, I'll use Lasso regularization to reduce the number of predictors in the model.

### Feature selection: LASSO

**Result:**

-   most important features:
-   25 predictors selected - 16 predictor variables (some of these are factors and thus accounted individually)

```{r, fig.height =12 , fig.width=10}
library(glmnet)
set.seed(42)

# Prepare the data for LASSO
# centering & scaling numeric variables
x <- cbind(trainData[, -c(1:3)] %>% 
             select(!where(is.numeric)), 
           scale(trainData[, -c(1:3)] %>% 
                   select(where(is.numeric)), center=T, scale = T))
x <- makeX(x, na.impute = T, sparse = T)  # Exclude the response variable; transforms factors to individual predictors

y <- trainData$log_R2_1 # Response variable

# Apply LASSO using cross-validation to find the optimal lambda
cv_lasso <- cv.glmnet(x, y, alpha = 1)

# Extract the best lambda value
best_lambda <- cv_lasso$lambda.min

# Fit the LASSO model with the best lambda
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)

# Identify the non-zero coefficients (rounded to 4 digests)
selected_features <- coef(lasso_model)[,1] %>% 
  as.data.frame() %>% round(4) %>%
  filter(. != 0)
selected_features$var <- row.names(selected_features)
selected_features <- selected_features[-1,]
  
# Print the selected features
print(selected_features)


ggplot(selected_features, aes(y = var, x = ., col = .))+
  geom_point()+
  theme_bw()+
  scale_color_distiller(palette = 4, type = "div", direction = 1)+
  labs(title = "Estimated coefficients (GLM)")





#### 
set.seed(42)

# Prepare the data for LASSO
# centering & scaling numeric variables
x0 <- cbind(trainData[, -c(1:3)] %>% 
             select(!where(is.numeric)), 
           scale(trainData[, -c(1:3)] %>% 
                   select(where(is.numeric)), center=T, scale = T))
x0 <- makeX(x0, na.impute = T, sparse = T)  # Exclude the response variable; transforms factors to individual predictors

y0 <- trainData$log_R2_1 # Response variable

# Apply LASSO using cross-validation to find the optimal lambda
cv_lasso0 <- cv.glmnet(x0, y0, alpha = 0)

# Extract the best lambda value
best_lambda0 <- cv_lasso0$lambda.min

# Fit the LASSO model with the best lambda
lasso_model0 <- glmnet(x, y, alpha = 0, lambda = best_lambda0)

# Identify the non-zero coefficients (rounded to 4 digests)
selected_features0 <- coef(lasso_model0)[,1] %>% 
  as.data.frame() %>% round(4) %>%
  filter(. != 0)
selected_features0$var <- row.names(selected_features0)
selected_features0 <- selected_features0[-1,]
  
# Print the selected features
print(selected_features0)


ggplot(selected_features0, aes(y = var, x = ., col = .))+
  geom_point()+
  theme_bw()+
  scale_color_distiller(palette = 4, type = "div", direction = 1)+
  labs(title = "Estimated coefficients (GLM)")





```

## Full model

```{r}
trainData1 <- trainData %>% select(-verbatim_name, -Telfer_1_2)
testData1 <- testData %>% select(-verbatim_name, -Telfer_1_2)

# Hyperparameter tuning
tunedGrid <- expand.grid(
  mtry = c(1:72),
  splitrule = c('variance', 'extratrees'),
  min.node.size = c(5))

# Cross-Validation
trainedControl <- trainControl(method = "repeatedcv", 
                              number = 3, # 3 splits
                              repeats = 5, # 10 repeats
                              savePredictions = T)

tictoc::tic()
# Train the random forest model
model_rf0 <- train(
  log_R2_1 ~ .,                  
  data = trainData1, 
  # random forest model from ranger package
  method = "ranger",
  # Cross-validation & tuning
  trControl = trainedControl, 
  tuneGrid = tunedGrid,
  respect.unordered.factors=TRUE,
  # Number of trees to grow
  num.trees = 1000,
  # local variable importance
  importance = 'permutation',  
  local.importance = T,
  oob.error = T,
  verbose = T)
tictoc::toc()

model_rf0$finalModel # mtry = 20, splitrule = extratrees, min.node.size = 5 ## R2 = 0.178, OOB-MSE = 0.246
model_rf0$finalModel[21]

# Check Model hyperparameter performance:
plot(model_rf0)

# Variable importance
plot(varImp(model_rf0))


# Predictions
predictions <- predict(model_rf0, newdata = testData1)

dd <- data.frame(testData1$log_R2_1, predictions)

plot(testData1$log_R2_1, predictions, type ='p')


# R2 values ====
# OOB Error:
model_rf0$finalModel[13] # OOB R2
# Test error:
postResample(pred = predictions, obs = testData1$log_R2_1)


dd %>%
ggplot(aes(x = testData1.log_R2_1, y = predictions)) +
  geom_point(shape = "circle", size = 1.5) +
  geom_smooth()

saveRDS(model_rf0, "script_clean/Assignment/model_rf0.rds")

```

## Reduced model

```{r}
# We will keep 'dataset' as well, as it is important to distinguish different atlases and thus take into account important regional differences that can influence the result

trainData2 <- trainData %>%
  select(log_R2_1, dataset, AlphaSR_sp, D_AOO_a, IUCN, 
         moran, Habitat, Trophic.Level, Trophic.Niche, Primary.Lifestyle, FP, HWI, 
         elonMinRect, Southernness, rel_maxDist, rel_elonRatio, rel_relCirc, rel_lin) 
testData2 <- testData %>%
   select(log_R2_1, dataset, AlphaSR_sp, D_AOO_a, IUCN, 
         moran, Habitat, Trophic.Level, Trophic.Niche, Primary.Lifestyle, FP, HWI, 
         elonMinRect, Southernness, rel_maxDist, rel_elonRatio, rel_relCirc, rel_lin) 
```

```{r}
# Hyperparameter tuning
tunedGrid <- expand.grid(
  mtry = c(1:44),
  splitrule = c('variance', 'extratrees'),
  min.node.size = c(5))

# Cross-Validation
trainedControl <- trainControl(method = "repeatedcv", 
                              number = 3, # 3 splits
                              repeats = 5, # 10 repeats
                              savePredictions = T)

tictoc::tic()
# Train the random forest model
model_rf <- train(
  log_R2_1 ~ .,                  
  data = trainData2, 
  # random forest model from ranger package
  method = "ranger",
  # Cross-validation & tuning
  trControl = trainedControl, 
  tuneGrid = tunedGrid,
  # Number of trees to grow
  num.trees = 1000,
  # local variable importance
  importance = 'permutation',  
  local.importance = T,
  oob.error = T,
  verbose = T)
tictoc::toc()

model_rf$finalModel # mtry = 20, splitrule = extratrees, min.node.size = 5 ## R2 = 0.178, OOB-MSE = 0.246


# Check Model hyperparameter performance:
plot(model_rf)

# Variable importance
plot(varImp(model_rf))


# Predictions
predictions <- predict(model_rf, newdata = testData2)

dd <- data.frame(testData2$log_R2_1, predictions)

plot(testData2$log_R2_1, predictions, type ='p')
dd %>%
ggplot(aes(x = testData2.log_R2_1, y = predictions)) +
  geom_point(shape = "circle", size = 1.5) +
  geom_smooth()
saveRDS(model_rf, "script_clean/Assignment/model_rf.rds")
```

## Model performance

```{r}
# Checking model performance ====

# 1. Model error plot
plot(rfFit1)

# Cross-Validation seems to overfit towards including all predictors at the same time ?

## adjust hyperparameters and repeat model:
rfFit1 <- ranger(log_R2_1 ~ .,
                 data = train_dat1_pp,
                 num.trees = 1000,
                 mtry = 19,
                 min.node.size = 5, 
                 splitrule = 'extratrees',
                 importance = 'permutation',
                 local.importance = T,
                 write.forest = T)
# 2. Predicted vs. Observed plot
res <- dat1_processed %>% select(verbatim_name)

# 3. Variable importance
plot(varImp(rfFit1, scale = T))

# 4. Feature Selection
y <- dat1_filtered %>% na.omit() %>% pull(log_R2_1)
x <- dat1_filtered %>% na.omit() %>% select(-log_R2_1, -verbatim_name)




ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)
subsets = c(1:43)

lmProfile <- rfe(trainData1[,-1], trainData1[,1],
                 sizes = subsets,
                 rfeControl = ctrl)

predictors(lmProfile)
lmProfile$variables
lmProfile$results
trellis.par.set(caretTheme())
plot(lmProfile, type = c("g", "o"))


xyplot(lmProfile$results$RMSE + lmProfile2$results$RMSE  ~
       lmProfile$results$Variables,
       type = c("g", "p", "l"),
       auto.key = TRUE)

```

# Model tuning with resampling loops

```{r}
# step 1. visualize raw data
# step 2. pre-process predictors (imputation, nzv, correlations)
# step 3. training / testing split with resampling

models_list <- vector("list", length = 10) 
 
set.seed(42)

# Create training (80%) and testing (20%) sets
trainIndex <- createDataPartition(dat1_v2$log_R2_1, list = FALSE, p = 0.8, 5) # 5 resamples

## step 3.1: for i in resamples *do*:
for (i in seq_along(1:ncol(trainIndex))){
  models_list[[i]] <- list()
  trainData <- dat1_v2[trainIndex[,i], ] # first resample
  testData <- dat1_v2[-trainIndex[,i], ] # first resample
  
  ### 1. tune / train data
  trainData1 <- trainData %>% select(-verbatim_name, -Telfer_1_2)
  testData1 <- testData %>% select(-verbatim_name, -Telfer_1_2)

  #### Hyperparameter tuning
  tunedGrid <- expand.grid(
    mtry = c(1:72),
    splitrule = c('variance', 'extratrees'),
    min.node.size = c(5))

  #### Cross-Validation
  trainedControl <- trainControl(method = "repeatedcv", 
                              number = 3, # 3 splits
                              repeats = 5, # 10 repeats
                              savePredictions = T)

  tictoc::tic()
  #### Train the random forest model
  model_rf0 <- train(
    log_R2_1 ~ .,                  
    data = trainData1, 

    # random forest model from ranger package
    method = "ranger",
    respect.unordered.factors=TRUE,
    oob.error = T,

    # Number of trees to grow
    num.trees = 1000,

    # local variable importance
    importance = 'impurity',
    #importance = 'permutation',  
    #local.importance = T,
    
    # Cross-validation & tuning
    trControl = trainedControl, 
    tuneGrid = tunedGrid,
    verbose = T)
  tictoc::toc()
  n_pred_vars <- ncol(trainData1)-1
  n_pred_all <- 72
  
  models_list[[i]]$full_model <- model_rf0
  
  ### 2. predict held-back samples
  # Predictions
  predictions <- predict(model_rf0, newdata = testData1)
  pred <- data.frame(testData1$log_R2_1, predictions)
  models_list[[i]][["prediction"]] <- pred

  ### 3. variable importance - rank predictors
  imp <- varImp(model_rf0, scale = T)$importance
  str(imp)
  imp <- imp[order(imp$Overall, decreasing = TRUE), , drop = FALSE]
  imp$var <- rownames(imp)
  rownames(imp) <- NULL
  models_list[[i]][["importance"]] <- imp

  ### 4. define list of # predictors (Si) that should be tried
  S <- seq(from = n_pred_all-1, to = 1) # total predictors = 72
  
  #### step 3.4.1 for j in Si *do*:
  for (j in seq_along(S)){
    ##### 1. keep Si most important predictors

    pred_red <- imp[1:j,]$var

    #  Function to check if one string starts with another string
    starts_with <- function(long, short) {
      stringr::str_starts(long, short)
    }

    # Create a dataframe with all combinations, including duplicates
    combinations <- expand.grid(long = pred_red, 
                                short =  names(testData1[,-1]), 
                                stringsAsFactors = FALSE)

# Filter combinations where the long string starts with the short string
    matches <- combinations %>%
      filter(starts_with(long, short)) %>%
      distinct()  # Remove duplicates
    pred_red2 <- unique(matches$short)
    trainData2 <- trainData1 %>% select(log_R2_1, all_of(pred_red2))
    testData2 <- testData1 %>% select(log_R2_1, all_of(pred_red2))
  
    # Hyperparameter tuning
    tunedGrid <- expand.grid(
      mtry = 1:j,
      splitrule = c('variance', 'extratrees'),
      min.node.size = c(5))

    # Cross-Validation
    trainedControl <- trainControl(method = "repeatedcv", 
                              number = 3, # 3 splits
                              repeats = 5, # 10 repeats
                              savePredictions = T)

    tictoc::tic()
    # Train the random forest model
    model_rf1 <- train(
      log_R2_1 ~ .,                  
      data = trainData2, 
      # random forest model from ranger package
      method = "ranger",
      # Cross-validation & tuning
      trControl = trainedControl, 
      tuneGrid = tunedGrid,
      respect.unordered.factors=TRUE,
      # Number of trees to grow
      num.trees = 1000,
      # local variable importance
      importance = 'permutation',  
      local.importance = T,
      oob.error = T,
      verbose = T)
    tictoc::toc()
  
    models_list[[i]][[y]][["model"]] <- model_rf1

    ##### 2. predict held-back samples
    prediction2 <- predict(model_rf1, testData2)
    pred2 <- data.frame(testData2$log_R2_1, predictions2)
    models_list[[i]][[y]][["prediction"]] <- pred2

    ##### 3. re-calculate rankings for each predictor  
    imp2 <- varImp(model_rf1)
    imp2 <- imp2[order(imp2$Overall,decreasing = TRUE),,drop = FALSE]
    imp2$var <- rownames(imp2)
    models_list[[i]][[y]][["importance"]] <- imp2
  
    }
  
  }

# step 4. check performance over Si with held-back samples
  # OOB Error:
OOB_err <-   models_list[[i]][["full_model"]]$finalModel[13] # OOB R2
  # Test error:
Test_err <- postResample(pred = predictions, obs = testData1$log_R2_1)


# step 5. determine best number of predictors
# step 6. estimate final list of predictors to keep in final model
# step 7. fit the final model with the optimal S using the original training set

```

```{r}
# Load necessary libraries
library(caret)
library(ranger)
library(dplyr)
library(tictoc)

# Assuming dat1_v2 is already loaded with the required dataset

# Create a list to store models and results
models_list <- vector("list", length = 5)

set.seed(42)

# Create training (80%) and testing (20%) sets with 5 resamples
trainIndex <- createDataPartition(dat1_v2$log_R2_1, list = FALSE, p = 0.8, times = 5)

## Step 3.1: For each resample, perform training and testing
for (i in seq_along(1:ncol(trainIndex))) {
  models_list[[i]] <- list()
  
  trainData <- dat1_v2[trainIndex[, i], ]
  testData <- dat1_v2[-trainIndex[, i], ]
  
  # Prepare training and testing data
  trainData1 <- trainData %>% select(-verbatim_name, -Telfer_1_2)
  testData1 <- testData %>% select(-verbatim_name, -Telfer_1_2)

  # Hyperparameter tuning grid
  tunedGrid <- expand.grid(
    mtry = 1:72,
    splitrule = c('variance', 'extratrees'),
    min.node.size = 5
  )

  # Cross-validation control
  trainedControl <- trainControl(
    method = "repeatedcv",
    number = 3, # 3 splits
    repeats = 5, # 5 repeats
    savePredictions = TRUE
  )

  # Train the random forest model
  tictoc::tic()
  model_rf0 <- train(
    log_R2_1 ~ .,                  
    data = trainData1,
    method = "ranger",
    respect.unordered.factors = TRUE,
    oob.error = TRUE,
    num.trees = 1000,
    importance = 'permutation',  
    local.importance = TRUE,
    #trControl = trainedControl, 
    #tuneGrid = tunedGrid,
    verbose = TRUE
  )
  tictoc::toc()

  n_pred_vars <- ncol(trainData1)-1
  n_pred_all <- 72
  models_list[[i]]$full_model <- model_rf0

  # Predict held-back samples
  predictions <- predict(model_rf0, newdata = testData1)
  pred <- data.frame(Observed = testData1$log_R2_1, Predicted = predictions)
  models_list[[i]]$prediction <- pred

  # Variable importance - rank predictors
  imp <- varImp(model_rf0)$importance
  imp <- imp[order(imp$Overall, decreasing = TRUE),, drop = FALSE]
  imp$var <- rownames(imp)
  models_list[[i]]$importance <- imp

  # Define list of predictors (Si) to be tried
  S <- seq(from = n_pred_all-1, to = 1) # total predictors = 72
  
  #### step 3.4.1 for j in Si *do*:
  for (j in seq_along(S)){
    ##### 1. keep Si most important predictors

    pred_red <- imp[1:j,]$var

    #  Function to check if one string starts with another string
    starts_with <- function(long, short) {
      stringr::str_starts(long, short)
    }

    # Create a dataframe with all combinations, including duplicates
    combinations <- expand.grid(long = pred_red, 
                                short =  names(testData1[,-1]), 
                                stringsAsFactors = FALSE)

# Filter combinations where the long string starts with the short string
    matches <- combinations %>%
      filter(starts_with(long, short)) %>%
      distinct()  # Remove duplicates
    pred_red2 <- unique(matches$short)
    trainData2 <- trainData1 %>% select(log_R2_1, all_of(pred_red2))
    testData2 <- testData1 %>% select(log_R2_1, all_of(pred_red2))
  
    # Hyperparameter tuning
    tunedGrid <- expand.grid(
      mtry = 1:j,
      splitrule = c('variance', 'extratrees'),
      min.node.size = c(5))

    # Cross-Validation
    trainedControl <- trainControl(method = "repeatedcv", 
                              number = 3, # 3 splits
                              repeats = 5, # 10 repeats
                              savePredictions = T)

    tictoc::tic()
    # Train the random forest model
    model_rf1 <- train(
      log_R2_1 ~ .,                  
      data = trainData2, 
      # random forest model from ranger package
      method = "ranger",
      # Cross-validation & tuning
      #trControl = trainedControl, 
      #tuneGrid = tunedGrid,
      respect.unordered.factors=TRUE,
      # Number of trees to grow
      num.trees = 1000,
      # local variable importance
      importance = 'permutation',  
      local.importance = T,
      oob.error = T,
      verbose = T)
    tictoc::toc()


    models_list[[i]][[j]] <- list(model = model_rf1)

    # Predict held-back samples
    predictions2 <- predict(model_rf1, newdata = testData2)
    pred2 <- data.frame(Observed = testData2$log_R2_1, Predicted = predictions2)
    models_list[[i]][[j]]$prediction <- pred2

    # Recalculate rankings for each predictor
    imp2 <- varImp(model_rf1)$importance
    imp2 <- imp2[order(imp2$Overall, decreasing = TRUE),, drop = FALSE]
    imp2$var <- rownames(imp2)
    models_list[[i]][[j]]$importance <- imp2
  }
}

# Step 4: Check performance over Si with held-back samples
for (i in seq_along(models_list)) {
  # OOB Error
  OOB_err <- models_list[[i]]$full_model$finalModel$prediction.error
  models_list[[i]]$full_model$OOB_error <- OOB_err
  
  # Test error
  Test_err <- postResample(pred = models_list[[i]]$prediction$Predicted, obs = models_list[[i]]$prediction$Observed)
  models_list[[i]]$full_model$Test_error <- Test_err
}


# Step 5: Determine Best Number of Predictors
best_num_predictors <- vector("numeric", length = length(models_list))
for (i in seq_along(models_list)) {
  errors <- sapply(models_list[[i]], function(x) {
    if (is.list(x) && !is.null(x$Test_error)) {
      return(x$Test_error[1])  # Use RMSE or any relevant metric
    } else {
      return(NA)
    }
  })
  best_num_predictors[i] <- which.min(errors)
}

# The most frequent best number of predictors across resamples
optimal_S <- round(mean(best_num_predictors, na.rm = TRUE))

# Step 6: Estimate Final List of Predictors
all_importances <- do.call(rbind, lapply(models_list, function(x) x$importance))
mean_importance <- aggregate(Overall ~ var, data = all_importances, mean)
mean_importance <- mean_importance[order(mean_importance$Overall, decreasing = TRUE),]

# Keep the top `optimal_S` predictors
final_predictors <- head(mean_importance$var, optimal_S)

# Step 7: Fit the Final Model with Optimal Predictors using the Original Training Set
trainIndex <- createDataPartition(dat1_v2$log_R2_1, list = FALSE, p = 0.8)
trainData <- dat1_v2[trainIndex, ]
testData <- dat1_v2[-trainIndex, ]

trainData_final <- trainData %>% select(all_of(final_predictors))
testData_final <- testData %>% select(all_of(final_predictors))

# Hyperparameter tuning grid for final model
tunedGrid_final <- expand.grid(
  mtry = 1:length(final_predictors),
  splitrule = c('variance', 'extratrees'),
  min.node.size = 5
)

# Cross-validation control for final model
trainedControl_final <- trainControl(
  method = "repeatedcv",
  number = 3, # 3 splits
  repeats = 5, # 5 repeats
  savePredictions = TRUE
)

# Train the final random forest model
tictoc::tic()
final_model <- train(
  log_R2_1 ~ .,                  
  data = trainData_final,
  method = "ranger",
  trControl = trainedControl_final, 
  tuneGrid = tunedGrid_final,
  respect.unordered.factors = TRUE,
  num.trees = 1000,
  importance = 'permutation',  
  local.importance = TRUE,
  oob.error = TRUE,
  verbose = TRUE
)
tictoc::toc()

# Evaluate the final model on the test data
final_predictions <- predict(final_model, newdata = testData_final)
final_performance <- postResample(pred = final_predictions, obs = testData$log_R2_1)

# Print final performance metrics
print(final_performance)

```

# rfe function

```{r}
rm(llist = ls())
library(caret)
library(dplyr)
load("Assignment_ws_rfFuncs.RData")

# Full data = dat1_v2
# Split data = trainData / testData

trainData0 <- trainData %>% select(-verbatim_name, -Telfer_1_2) 
testData0 <- testData %>% select(-verbatim_name, -Telfer_1_2) 

# 4. Feature Selection (full data)
y0 <- dat1_v2 %>%
    na.omit() %>%
    pull(log_R2_1)
x0 <- dat1_v2 %>%
    na.omit() %>%
    select(-log_R2_1, -Telfer_1_2, -verbatim_name)

# 4.2 Feature selection (split data)
ys <- trainData0 %>% na.omit() %>% pull(log_R2_1)
xs <- trainData0 %>% na.omit() %>% select(-log_R2_1)

# Lists with data to loop through (full data, split data)
y_l <- list(y0, ys)
x_l <- list(x0, xs)

# Control for recursive feature selection
set.seed(42)
ctrl <- rfeControl(
    functions = rfFuncs,
    returnResamp = T,
    method = "repeatedcv",
    repeats = 5,
    verbose = FALSE
)

subsets <- c(1:72) # 1: max number of predictive variables (incl. dummy vars)
out_list <- vector("list", length = 2)

for (i in seq_along(1:2)){
  # Recursive Features selection
  rfProfile <- rfe(x_l[[i]], y_l[[i]],
    sizes = subsets,
    rfeControl = ctrl)
  
  # Results
  out_list[[i]] <- list()
  out_list[[i]]$object <- rfProfile
  out_list[[i]]$predictors <- predictors(rfProfile)
  out_list[[i]]$variables <- rfProfile$variables
  out_list[[i]]$topVars <- out_list[[i]]$variables %>% 
    group_by(var) %>% 
    summarise(Mean_resamp = mean(Overall)) %>% 
    arrange(desc(Mean_resamp), decreasing = T)
  out_list[[i]]$results <- rfProfile$results

  # Plots
  trellis.par.set(caretTheme())
  out_list[[i]]$plot1 <- plot(rfProfile, type = c("g", "o"))
  out_list[[i]]$plot2 <- xyplot(
    rfProfile$results$Rsquared + rfProfile$results$RMSE ~
        rfProfile$results$Variables,
    type = c("g", "p", "l"))
    
    }

saveRDS(out_list, "script/script_clean/Assignment/out_list_FeatureSelection.rds")
```

```{r}
library(dplyr)
out_list <- readRDS("script/script_clean/Assignment/out_list_FeatureSelection.rds")

top_vars_full <- out_list[[1]]$object$optVariables # 43 vars
keep <- c(out_list[[2]]$object$optVariables)

imp_full <- out_list[[1]]$topVars
names(imp_full) <- c("full_data", "full_data_imp")
imp_resamp <- out_list[[2]]$topVars
names(imp_resamp) <- c("resamps", "resamp_mean_imp")

top_vars <- data.frame(rank = seq(1:43),
                      full_data = top_vars_full, 
                      resamps = imp_resamp$resamps) 

top_vars <- top_vars %>% 
  left_join(imp_full) %>%
  full_join(imp_resamp) %>%
  mutate(keep_resamp = 
      case_when(resamps %in% keep ~ 1,
      .default = 0))

write.csv(top_vars, "out/csv/Top_vars_resamp_summary.csv")
```

# Final model (38 predictors)

```{r}
library(caret)
library(caretEnsemble)
library(dplyr)
library(ranger)

# Load Workspace
load("Assignment_ws_rfFuncs.RData")

# Preprocess the data
dat1_v3 <- dat1_v2 %>%
  na.omit() %>%
  select(log_R2_1, all_of(keep))

## Loop through 5 resamples of training&testing splits:
# Create training (80%) and testing (20%) sets
set.seed(42)
train_index <- createDataPartition(
  dat1_v3$log_R2_1,
  p = 0.8,
  list = TRUE,
  times = 5
)

## 10-fold cross-validation with 3 repeats:
# Define training control
train_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  savePredictions = "final",
  index = train_index  # For 5 resamples of data splitting
)

# Try adaptive algorithm for faster results:
train_control_fast <- trainControl(
  method = "adaptive_cv",
  number = 10,
  repeats = 5,
  preProcOptions = c('corr'),
  index = train_index, 
  adaptive = list(min = 3, 
                  alpha = 0.05, 
                  method = "gls",
                  complete = TRUE))

# Define the list of models (with extensive automatized grid search via tuneLength = 10)
set.seed(42)
model_list_fast <- caretList(
  log_R2_1 ~ .,
  data = dat1_v3,
  metric = "RMSE",
  trControl = train_control_fast,
  continue_on_fail = TRUE,
  tuneList = list(
    rf = caretModelSpec(
      method = "ranger",
      importance = "permutation",
      local.importance = TRUE,
      oob.error = TRUE,
      respect.unordered.factors = TRUE,
      num.trees = 1000,
      tuneLength = 3),
    gbm = caretModelSpec(
      method = "gbm",
      distribution = "gaussian",
      keep.data = TRUE,
      tuneGrid = expand.grid(
        n.trees = c(150, 500, 1000),
        interaction.depth = c(1:5),
        shrinkage = seq(from = 0.001, to = 0.1, by = 0.005),
        n.minobsinnode = c(1:10))),
    xgboost = caretModelSpec(
      method = "xgbTree",
      tuneLength = 3
                    )
                  )
                )
set.seed(42)
model_list <- caretList(
  log_R2_1 ~ .,
  data = dat1_v3,
  metric = "RMSE",
  trControl = train_control,
  continue_on_fail = TRUE,
  tuneList = list(
    rf = caretModelSpec(
      method = "ranger",
      importance = "permutation",
      local.importance = TRUE,
      oob.error = TRUE,
      respect.unordered.factors = TRUE,
      num.trees = 1000,
      tuneLength = 3),
    gbm = caretModelSpec(
      method = "gbm",
      distribution = "gaussian",
      keep.data = TRUE,
      tuneGrid = expand.grid(
        n.trees = c(150, 500, 1000),
        interaction.depth = c(1:5),
        shrinkage = seq(from = 0.001, to = 0.1, by = 0.005),
        n.minobsinnode = c(1:10))),
    xgboost = caretModelSpec(
      method = "xgbTree",
      tuneLength = 3
                    )
                  )
                )

plot(model_list$rf)
plot(model_list$gbm)
plot(model_list$xgboost)

```

```{r}
sessionInfo() %>% str()
Packages_citation <- lapply(sessionInfo()$otherPkgs %>% names(), citation)
print(Packages_citation)
```
