---
title: "Random Forest Models"
author: "Friederike WÃ¶lke"
date: "2024-04-25"
output:
  html_document:
    code_folding: hide
format:
  html:
    toc: true
    number-sections: true
    colorlinks: true
---

```{r}
library(dplyr)
library(caret)
library(caretEnsemble)

dat <- readRDS("../../out/rds/Final_data.rds")

```

```{r}
library(randomForestSRC)
df<- dat
str(df)

df$tp <- as.factor(as.character(df$tp))

dta <- tidyr::gather(df, "variable", "value", -log_R2_1,  -verbatim_name,  -tp, -dataset)
dta <- tidyr::gather(df[1:25] %>% select(-Telfer_1_2), "variable", "value", -log_R2_1, -verbatim_name, -tp, -dataset)

dta %>% filter(tp == 1) %>%
ggplot() +
  geom_point(alpha = 0.4, aes(y = log_R2_1, x = value, color = dataset)) +
  facet_wrap(~ variable, scales = "free_x", ncol = 4)


## Remove highly correlated:
cor_df <- df %>% select(-log_R2_1, -Telfer_1_2, -verbatim_name)
p.mat <-  model.matrix(~0+., data=cor_df) %>% ggcorrplot::cor_pmat()
correlation_matrix <- model.matrix(~0+., data=cor_df) %>% 
  cor(use="pairwise.complete.obs") %>% ggcorrplot::ggcorrplot()
# Filter correlations greater than 0.8 or less than -0.8
high_correlations <- which(abs(correlation_matrix) > 0.8 & correlation_matrix != 1, arr.ind = TRUE)

# Extract the highly correlated pairs
high_correlation_pairs <- high_correlations[order(abs(correlation_matrix[high_correlations]), decreasing = TRUE), ]

# Print highly correlated pairs and their correlations
for (i in 1:nrow(high_correlation_pairs)) {
  row_idx <- high_correlation_pairs[i, 1]
  col_idx <- high_correlation_pairs[i, 2]
  correlation <- correlation_matrix[row_idx, col_idx]
  cat("Variables:", rownames(correlation_matrix)[row_idx], "and", 
      colnames(correlation_matrix)[col_idx], "Correlation:", correlation, "\n")
}

## Exclude them:
exclude_vars <- c("m_AOO_a", "Total_area", "Total_Ncells", "rel_occ_Ncells", "Gamma", "Range.Size", "GlobRangeSize_m2", "mean_cell_length", "increment2", "RL_Category", "atlas_xmax", "area", "b_AOO_a", "mean_relAOO", "occ_Ncells", "mean_prob_cooccur")

atlas_vars <- c("Total_area_samp", "GammaSR", "mean_area", "atlas_lengthMinRect", "atlas_widthMinRect", "atlas_elonMinRect", "atlas_circ", "atlas_bearingMinRect", "atlas_bearing", "AtlasCOG_long", "AtlasCOG_lat")

df2 <- df %>% select(!all_of(c(exclude_vars, atlas_vars)))

## Find top variables in the dataset:
o <- max.subtree(rfsrc(log_R2_1 ~., df2 %>% select(-Telfer_1_2, -verbatim_name), na.action = "na.impute"))
o$topvars
```

```{r}


colSums(is.na(df2))

# Manual NA fixes:
df3 <- df2 %>% 
group_by(dataset, tp, verbatim_name) %>% 
mutate(
    D_AOO_a = case_when(is.na(D_AOO_a) ~ 2,
    .default = D_AOO_a),
    IUCN = case_when(is.na(IUCN) ~ "DD", 
    .default = IUCN))
colSums(is.na(df3))

df3 %>% group_by(dataset, tp) %>% 
  summarise(across(everything(), ~ sum(is.na(.x)))) %>% View()


model_df <- df3 %>% na.omit()

```

```{r}

# Create grouping variables for modelling in loop:
cols <- names(model_df) # 47 vars (incl. 2x response + verbatim_name)

# Response:
cols_logR <- cols[-which(cols == "Telfer_1_2")]
cols_telfer <- cols[-which(cols == "log_R2_1")]

# Group for predictor columns:
# Geometry
cols_g1 <- c("D_AOO_a", # Fractal dimension
            "Total_Ncells_samp",
             "rel_nsDist", "rel_maxDist", "rel_ewDist",
             "rel_elonRatio", "rel_circNorm", "rel_relCirc", "rel_lin",  # Species distribution geometric features
             "minDist_toBorder_border", "minDist_toBorder_centr", # Distances
             "maxDist_toBorder_border", "maxDist_toBorder_centr",
             "bearingMinRect", "bearing", "elonMinRect","circ",
             "lengthMinRect","widthMinRect",
             "Southernness", "Westernness", 
             "moran", "x_intercept", # Spatial autocorrelation
             # "atlas_circNorm", "atlas_relCirc", # Atlas predictors
             "dataset") # captures most of the geometry of the atlases (highly correlated)

# Traits
cols_g2 <- c("rel_AOO", # Occupancy 
             "AOO",
             "IUCN", # Global red list categories
             "FP", # Evolutionary relationships
             "HWI", # Dispersal
             "Mass", 
             "Habitat", "Habitat.Density", "Migration", "Trophic.Level", "Trophic.Niche", "Primary.Lifestyle", # Life history traits
             "GlobRangeSize_km2", # Global range size (Breeding & Resident)
             "sd_PC1", "sd_PC2") # Climatic Niche Breadth

# Diversity
cols_g3 <- c("AlphaSR_sp", "BetaSR_sp", #Alpha and beta diversity
             "Dist_centroid_to_COG",  # Distance from species centroid to center of gravity of the atlas
             # "CenterOfGravity_Atlas_lat", # latitude of center of gravity of the atlas
             "sp_centr_lon","sp_centr_lat") # species distribution centroid coordinates) # Distance from species centroid to center of gravity of the atlas


# Which columns are not assigned yet?
cols2 <- setdiff(cols, cols_g1)
cols3 <- setdiff(cols2, cols_g2)
cols_not_used <- setdiff(cols3, cols_g3)
```

```{r}
# Log Ratio df
df_log_R <-  model_df %>% 
  select(all_of(cols_logR)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(log_R2_1))

df_log_R1 <- df_log_R %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_log_R2 <- df_log_R %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change

# Telfer df
df_telfer <- model_df %>% 
  select(all_of(cols_telfer)) %>%
  distinct(tp, dataset, verbatim_name, .keep_all = T) %>%
  filter(!is.na(Telfer_1_2))

df_telfer1 <- df_telfer %>% filter(tp == 1) %>% select(-tp, -verbatim_name) # Future Change
df_telfer2 <- df_telfer %>% filter(tp == 2) %>% select(-tp, -verbatim_name) # Past Change



# Create list of data frame:
dfs_list <- list(df_log_R1, df_log_R2, df_telfer1, df_telfer2)
names(dfs_list) <- c("df_log_R1", "df_log_R2", "df_telfer1", "df_telfer2")
```

# The Caret Package - Way

```{r}
set.seed(123)




for ()
inTrain <- createDataPartition(y = dd[response_var], p = 0.75) 
```





##### old

```{r}
library(randomForest)
# RMSE formula:
## RMSE is measure of average error; it has the same unit as the response
RMSE <- function(error){
    sqrt(mean(error^2))
} 

## Loop to calculate Performance across all splits and test sets for all times and both responses



PredPerformance_list <- list()
response <- c("log_R2_1", "log_R2_1", "Telfer_1_2", "Telfer_1_2")
for (i in seq_along(response)){
    response_df <- dfs_list[[i]]


    ## Splitting the data
    split <- round(nrow(response_df) * 0.80)
    train <- response_df[1:split, ]
    test <- response_df[(split + 1):nrow(response_df), ]
    
    response_var <- if ("log_R2_1" %in% names(train)) "log_R2_1" else if ("Telfer_1_2" %in% names(train)) "Telfer_1_2" 
    
    ## RMSE Calculations: 

    # In-sample Error: predicting with old data
    model <- randomForest(as.formula(paste(response_var, "~ .")), data = train)
    predicted <- predict(model, train, type = "response")
    names(predicted) <- NULL
    actual <- train[,response_var]
    error <- c(predicted-actual) %>% as.data.frame() %>% pull(response_var)
    in_sample_err <- RMSE(error)

    # Out-of-Sample error: # predicting 
    predicted <- predict(model, test, type = "response")
    actual <- test[,response_var]
    error <- c(predicted-actual) %>% as.data.frame() %>% pull(response_var)
    out_of_sample_err <- RMSE(error)

    PredPerformance <- data.frame(response = response_var,
               inBagErr = in_sample_err,
               outBagErr = out_of_sample_err)

    PredPerformance_list[[i]] <- PredPerformance

}
Pred_Performance <- do.call(rbind,PredPerformance_list)



```

### Cross-validation:

```{r}
library(ranger)
set.seed(123)

myControl <- trainControl(
                    method = "repeatedcv", 
                    number = 5, 
                    repeats = 5,
                    verboseIter = T)
model <- train(as.formula(paste("log_R2_1", "~ .")), 
                dfs_list[[1]] %>% filter(tp == 1), 
                method =  "ranger", 
                tuneLength = 5,
                preProcess = c("zv", "nzv", "medianImpute"),
                trControl = myControl)
plot(model)


model2 <- train(as.formula(paste("log_R2_1", "~ .")), 
                dfs_list[[1]], 
                method =  "ranger", 
                tuneLength = 10,
                trControl = trainControl(
                    method = "repeatedcv", 
                    number = 5,
                    repeats = 5, 
                    verboseIter = TRUE))
plot(model2)


```

## Hyperparameter tuning:

-   mtry = number of variables selected at each split
    -   lower values of mtry give more random trees

```{r}
n_obs <- nrow(dfs_list[[1]])
permuted_rows <- sample(n_obs)
my_data <- dfs_list[[1]][permuted_rows, ] # randomly reorder the data
split <- round(n_obs * 0.6)
train <- response_df[1:split, ]
test <- response_df[(split + 1):nrow(response_df), ]


```

## Build models

```{r}
library(randomForest)
# create empty lists
models_list <- list()
models_res_list <-list()
pred_list <- list()
pp_list <-list()

# loop through list of model dataframes
for (dd in seq_along(dfs_list)){
  model_df <- dfs_list[[dd]]
  
  # split into test and train #
  set.seed(123)
  samp <- sample(nrow(model_df), 0.8 * nrow(model_df))
  train <- model_df[samp, ]; dim(train) 
  test <- model_df[-samp, ]; dim(test) 

  # create response variable
  response_var <- if ("log_R2_1" %in% names(train)) "log_R2_1" else if ("Telfer_1_2" %in% names(train)) "Telfer_1_2" 
  
  # build models # 
  model <- randomForest(as.formula(paste(response_var, "~ .")), data = train,
                         ntree=1000, importance=TRUE, na.action = "na.omit")
  # predict test data #
  data_pred <- data.frame(Predicted = predict(model, newdata = test), Observed = test[[response_var]])
  
pp_list[[dd]] <- list(
partialPlot(model, train, x.var = D_AOO_a, ylim=c(-1,1)),
partialPlot(model, train, x.var = moran, ylim=c(-1,1)),
partialPlot(model, train, x.var = rel_AOO, ylim=c(-1,1)),
partialPlot(model, train, x.var = AlphaSR_sp, ylim=c(-1,1)),
partialPlot(model, train, x.var = x_intercept, ylim=c(-1,1)),
partialPlot(model, train, x.var = Mass, ylim=c(-1,1)))




  models_list[[dd]] <- model    
  pred_list[[dd]] <- data_pred
  
  # extract model results # 
  imp <- as.data.frame(model$importance[,1])
  names(imp) <- c("Importance")
  imp$var <- rownames(imp)
  imp2 <- imp %>% 
    tidyr::pivot_wider(names_from = var,
                       values_from = Importance)
  # create summary df
  model_res <- data.frame(
    df = names(dfs_list)[[dd]],
    mean_pseudo_OOB_R2 = model$rsq[1000],
    mean_OOB_mse = model$mse[1000],
    imp2) # add variable importance for each predictor
  
  models_res_list[[dd]] <- model_res
  
}


models_res_df <- plyr::rbind.fill(models_res_list)
par(mfrow=c(2,3));

lapply(pp_list[[2]], plot, type="l", ylim=c(0,1))


```

```{r, OOB error}
oob.err<-double(36)
test.err<-double(36)

obb.err_list <- list()
test.err_list <- list()

#mtry is no of Variables randomly chosen at each split
# loop through list of model dataframes
for (dd in seq_along(dfs_list[1:2])){
  model_df <- dfs_list[[dd]]
  
  # split into test and train #
  set.seed(123)
  train<-sample(1:nrow(model_df),503)

  # create response variable
  response_var <- if ("log_R2_1" %in% names(model_df)) "log_R2_1" else if ("Telfer_1_2" %in% names(train)) "Telfer_1_2" 
for(mtry in 1:36) {
  rf <- randomForest(as.formula(paste(response_var, "~ .")), data = model_df, subset = train, mtry=mtry, ntree=1000) 
  oob.err[mtry] = rf$mse[1000]  #Error of all Trees fitted
  
  pred<-predict(rf,model_df[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(model_df[-train,], mean((log_R2_1 - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ")
  
}
  
  obb.err_list[[dd]] <- oob.err
  test.err_list[[dd]] <- test.err
}



matplot(1:mtry , 
        cbind(obb.err_list[[1]],test.err_list[[1]]), pch=19 , col=c("red","blue"),
        type="b",
        ylab="Mean Squared Error",
        xlab="Number of Predictors Considered at each Split",
ylim = c(0,0.35), main = "Future Change")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))


matplot(1:mtry , 
        cbind(obb.err_list[[2]],test.err_list[[2]]), pch=19 , col=c("red","blue"),
        type="b",
        ylab="Mean Squared Error",
        xlab="Number of Predictors Considered at each Split",ylim = c(0,0.35), main = "Past Change")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))


saveRDS(obb.err_list, "OOB_Error.rds")
saveRDS(test.err_list, "Test_Error.rds")

```

## Quick visualization:

```{r}
# Decrease of OOB MSE (?)

mse_dfs <- list()
for (i in seq_along(models_list)){
  decrease_mse <- data.frame(Decr_OOB_MSE = importance(models_list[[i]], type = 1),
                            data = case_when(i == 1 ~ "log_R_future",
                                             i == 2 ~ "log_R_past",
                                             i == 3 ~ "telfer_future",
                                             i == 4 ~ "telfer_past"))
  decrease_mse$var <- rownames(decrease_mse)
  decrease_mse <- decrease_mse %>% mutate(group = case_when(
    var %in% cols_g1 ~ "geometry",
    var %in% cols_g2 ~ "traits",
    var %in% cols_g3 ~ "diversity")) %>% 
  rename("Decr_OOB_MSE" = "X.IncMSE")
  mse_dfs[[i]] <- decrease_mse
}

mse_df <- plyr::rbind.fill(mse_dfs)

# Figure:
mse_df %>% 
  group_by(data) %>%
  filter(data %in% c("log_R_future", "log_R_past")) %>%
  
  ggplot2::ggplot(aes(y = fct_reorder(var, Decr_OOB_MSE), x = Decr_OOB_MSE, fill = group))+
  geom_col()+
  facet_wrap(data~.)+
  xlab("Decrease of out-of-bag MSE")+
  ylab("Predictor")+
  theme_minimal()

```

```{r}
df_all <- df_log_R
df_f <- dfs_list[[1]]
df_p <- dfs_list[[2]]

all_model <- randomForest(log_R2_1 ~ ., data = df_all, na.roughfix = T)
varImpPlot(all_model)

f_model <- randomForest(log_R2_1 ~ ., data = df_f, na.roughfix = T)
varImpPlot(f_model)

p_model <- randomForest(log_R2_1 ~ ., data = df_p, na.roughfix = T)
varImpPlot(p_model)

```

```{r}
pacman::p_load(ggRandomForests, randomForestSRC)

str(df_f)
df_f$Order <- as.factor(df_f$Order)
## Determine mtry & nodesize:
o <- tune(log_R2_1 ~ ., df_f)
model <- rfsrc(log_R2_1 ~ ., data = df_f, ntree=1000, mtry = 22, nodesize = 3, na.action = "na.impute")

## Determine important vars:
o2 <- max.subtree(model)
o2$topvars


# Extract OOBs
x <- gg_rfsrc(model)
plot(x, alpha=.5)
y <- gg_error(model)
plot(y)


gg_dta <- gg_vimp(model) 
plot(gg_dta)

partial.obj <- partial.rfsrc(model, partial.xvar = "D_AOO_a", partial.values = model$xvar$D_AOO_a)
pdta <- get.partial.plot.data(partial.obj)
plot(pdta$x, pdta$yhat, type = "b", pch = 16,
      xlab = "D_AOO_a", ylab = "partial effect of D on Change")

partialPlot(model, train, x.var = moran, ylim=c(-1,1))
partialPlot(model, train, x.var = relative_occupancy_Ncells, ylim=c(-1,1))
partialPlot(model, train, x.var = AlphaSR_sp, ylim=c(-1,1))
partialPlot(model, train, x.var = x_intercept, ylim=c(-1,1))
partialPlot(model, train, x.var = Mass, ylim=c(-1,1))

```