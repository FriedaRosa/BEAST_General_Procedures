---
title: "Atlas Variables Calculations"
author: "Friederike WÃ¶lke"
date: "2024-04-20"
output:
  html_document:
    code_folding: hide
format:
  html:
    toc: true
    number-sections: true
    colorlinks: true
---

# Clean Environment
```{r, message = F}
rm(list=ls())
gc()
```

# Libraries
```{r, mesaage = F}
# Spatial:
library(sf) 
sf_use_s2(FALSE) # switch spherical geometry off

# Data handling:
library(rstatix)
library(dplyr) 
library(tidyr) 
# library(plyr) 
## plyr is required but produces some issues with dplyr. Thus it will be called in each function individually and is not required to load here.

# Telfer calculation:
## sparta is not on CRAN and cannot be updated as usual because there is another package called sparta from CRAN but we don't want this.
# library(devtools)
# install_github('biologicalrecordscentre/sparta')
# library(sparta) 

```

# Load Environment
```{r}
load("../../out/RData/1_Atlas_Data_clean.RData")
ls()
```


# Calculations
## 1. Occupancy
### 1.1 Calculate Areas from Atlases
```{r,mesaage = F, warning = F}
pres_dat_full_grain <- list()
pres_dat_full_atlas <- list()

for (n_atlas in seq_along(atlas_names)){
  atlas_name <- atlas_names[n_atlas] # set variable for current run of the loop
  atlas <- grid_list[[n_atlas]] # subset list of atlases to one
  
  for (grain in seq_along(atlas)){
    # set some variables for current run of the loop
    grain_a <- names(atlas)[grain]
    grain_d <- as.numeric(gsub("\\D", "", grain_a)) # "\\D" = remove all non-numbers
    
    # subset atlas to one grain
    atlas_1scale <- atlas[[grain]] %>% 
      select(cell_grouping, cell_label, area, cell_long, cell_lat) %>%
      st_drop_geometry()


    ## =============================================================== ###
    ## ================= Calculate Areas ============================= ###
    ## =============================================================== ###


    # Total Area and Total number of cells of the Atlas (as measured from grid)
    atlas_1scale <- atlas_1scale %>% 
      group_by(cell_grouping) %>%
      mutate(
        Total_area = sum(atlas_1scale$area),
        Total_Ncells = n_distinct(cell_label))
    
    # Total number of sampled cells and total Area sampled per sampling period
    Total_Ncells_samp <- presence_data_filt %>% 
      filter(dataset == atlas_name & cell_grouping == grain_d) %>%
      select(dataset, cell_grouping, cell_label, area) %>% unique() %>% na.omit() %>%
      group_by(dataset, cell_grouping) %>% distinct(cell_label, .keep_all = T) %>%
      summarise(
        n_cells = n_distinct(cell_label),
        area_samp = sum(area)) %>% 
      ungroup() %>%
      distinct(n_cells, area_samp)
    
    # Add columns to atlas
    atlas_1scale$Total_area_samp <- Total_Ncells_samp$area_samp
    atlas_1scale$Total_Ncells_samp <- Total_Ncells_samp$n_cells
    map_atlas_1scale <- atlas_1scale
    
    # subset the presence/absence data to the current spatial grain:
    pres_data_1atlas <- presence_data_filt %>% 
      filter(dataset == atlas_name) %>%
      filter(cell_grouping == grain_d) %>%
      select(dataset, tp, verbatim_name, cell_label, cell_grouping)
  
    # Add Areas to data
    pres_data_full_1atlas <- left_join(map_atlas_1scale, pres_data_1atlas)
    pres_data_full_1atlas %>% filter(!is.na(verbatim_name))
    pres_data_full_1atlas$dataset <- atlas_names[n_atlas]
    pres_dat_full_grain[[grain]] <- pres_data_full_1atlas %>% 
        unique() %>% 
        filter(!is.na(verbatim_name))
    
  }
  
  pres_dat_grain2 <- plyr::rbind.fill(pres_dat_full_grain, fill=T)
  pres_dat_full_atlas[[n_atlas]] <- pres_dat_grain2
  
}

# Bind list to dataframe
pres_dat_full <- plyr::rbind.fill(pres_dat_full_atlas, fill=T)
pres_dat_full[pres_dat_full == "<NA>"] = NA #Fix NA issues in the data (takes a while.)

# Add Column for grain size (in km, i.e., approx. length of the cell in the grid)
pres_dat_final <- pres_dat_full %>%  
  mutate(cell_grouping = factor(cell_grouping,levels = desired_levels),
        grain = case_when(
          dataset == atlas_names[1] ~ "10",
          dataset == atlas_names[2] ~ "5",
          dataset == atlas_names[3] ~ "20", 
          dataset == atlas_names[4] ~ "50")) %>% 
  filter(!is.na(verbatim_name))

### =============================================== ###
rm(pres_dat_full_atlas, pres_dat_full_grain, pres_dat_grain2, pres_dat_full, pres_data_1atlas, pres_data_full_1atlas, 
map_atlas_1scale, atlas_1scale,
Total_Ncells_samp, presence_data_filt, atlas, atlas_name, grain, grain_a, grain_d, n_atlas)
### =============================================== ###
jaccard <- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}
### 
Jac_list <-  vector("list", length = 4)
for (a in seq_along(atlas_names)){
  dd <- pres_dat_final %>% filter(dataset == atlas_names[a] & cell_grouping == 1)
  sp_list <- unique(dd$verbatim_name)
  
  for (sp in seq_along(sp_list)){
    dd2 <- dd %>% filter(verbatim_name == sp_list[sp])
    tp1 <- dd2 %>% filter(tp == 1)
    tp2 <- dd2 %>% filter(tp == 2)
    
    cells_1 <- unique(tp1$cell_label)
    cells_2 <- unique(tp2$cell_label)
    
    Jc_df <- data.frame(Jaccard = jaccard(cells_1, cells_2),
               verbatim_name = sp_list[sp],
               dataset = atlas_names[a])
    
    Jac_list[[a]][[sp]] <- Jc_df
    
    
  }
  
  
}

Jaccard_df <- do.call(rbind, unlist(Jac_list, recursive = F))
pres_dat_final <- pres_dat_final %>% left_join(Jaccard_df)
```


### 1.2 Calculate Occupancy 
```{r}
occ_data <- pres_dat_final %>%
  ungroup() %>% 
  distinct(dataset, tp, cell_grouping, verbatim_name, cell_label, .keep_all=T) %>%
  group_by(dataset, tp, cell_grouping, verbatim_name) %>% 

# Calculate AOO:
  mutate(
    mean_area = mean(area),
    AOO = sum(area),
    occ_Ncells = n_distinct(cell_label)) %>%

# Calculate relative Occupancy:
  mutate(
    rel_AOO = AOO/Total_area_samp,
    rel_occ_Ncells = occ_Ncells/Total_Ncells_samp) %>% 

  ungroup() %>% group_by(dataset, tp, cell_grouping) %>%

  mutate(
    Gamma = n_distinct(verbatim_name)) %>%
# Remove duplicated rows:
  distinct() 

# create scale column as a fraction of the full country:
occ_data_final <- occ_data %>% 
  ungroup() %>%
  group_by(dataset) %>%
  mutate(
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "1", 1/64, NA),
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "2", 1/32, scale), 
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "4", 1/16, scale), 
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "8", 1/8, scale),
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "16", 1/4, scale),
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "32", 1/2, scale),
    scale = ifelse(dataset %in% c(atlas_names[1]) & cell_grouping == "64", 1, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "1", 1/128, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "2", 1/16, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "4", 1/32, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "8", 1/16, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "16", 1/8, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "32", 1/4, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "64", 1/2, scale),
    scale = ifelse(dataset %in% c(atlas_names[2:4]) & cell_grouping == "128", 1, scale))

colSums(is.na(occ_data_final))


# save reduced version of this to file:

species_data <- occ_data_final %>% 
  select(
  dataset, tp, cell_grouping, verbatim_name, 
  mean_area, Total_area, Total_area_samp, Total_Ncells, Total_Ncells_samp, 
  Gamma,AOO, occ_Ncells, rel_occ_Ncells, rel_AOO, Jaccard) %>%
  distinct(dataset, tp, cell_grouping, verbatim_name, .keep_all = T) 

### =============================================== ###
rm(occ_data, pres_dat_final)
### =============================================== ###

```

## 2. Occupancy-Area-Relationship (OAR)
### 2.1 Mark saturated scales 
- code dummy column "exclude" to exclude species with < 2 scales not saturated (i.e, if less than two scales are available for OAR calculation)
- Saturation: rel_AOO = 1

```{r}
list_sp <- list()
list_tp <- list()
list_a <- list()

for (a in seq_along(atlas_names)){
  temp_df <- species_data %>% filter(dataset == atlas_names[a])

  for (t in seq_along(time_periods)) {
    temp_df_t <- temp_df %>% filter(tp == time_periods[t])
    species_names <- unique(temp_df_t$verbatim_name)

    for (s in seq_along(species_names)){
      temp_df_s <- temp_df_t %>% filter(verbatim_name == species_names[s])
      
      # Exclude saturated scales
      temp_df_red <- temp_df_s %>% filter(rel_AOO < 1)
      
      if (nrow(temp_df_red) < 2 & nrow(temp_df_red) > 0) {
        out_df <- data.frame(verbatim_name = species_names[s],
                             dataset = atlas_names[a],
                             tp = time_periods[t],
                             exclude = 1,
                             available_scales = nrow(temp_df_red),
                             mean_relAOO = mean(temp_df_red$rel_AOO, na.rm=T))
      } else if (nrow(temp_df_red) >= 2) {
        out_df <- data.frame(verbatim_name = species_names[s],
                             dataset = atlas_names[a],
                             tp = time_periods[t],
                             exclude = 0,
                             available_scales = nrow(temp_df_red),
                             mean_relAOO = mean(temp_df_red$rel_AOO, na.rm=T))
      } else {
                out_df <- data.frame(verbatim_name = species_names[s],
                             dataset = atlas_names[a],
                             tp = time_periods[t],
                             exclude = 1,
                             available_scales = nrow(temp_df_red),
                             mean_relAOO = 1)
                }
      list_sp[[s]] <- out_df
    
      }
    sp_df <- plyr::rbind.fill(list_sp)
    list_tp[[t]] <- sp_df
  
    }
  tp_df <- plyr::rbind.fill(list_tp)
  list_a[[a]] <- tp_df

}

atlas_df <- plyr::rbind.fill(list_a)

str(atlas_df)
str(species_data)
sp_data_new <- full_join(species_data, atlas_df) %>% distinct(dataset, tp, cell_grouping, verbatim_name, .keep_all=T)

### =============================================== ###
rm(a,t,s, temp_df, temp_df_red, temp_df_s, temp_df_t, atlas_df, species_data, sp_df, tp_df, list_a, list_sp, list_tp, out_df)
### =============================================== ###

```

### 2.2 Calculate OAR & Fractal Dimension
```{r}
dd <- sp_data_new %>%
  filter(exclude == 0) %>%
  filter(rel_occ_Ncells < 1) %>% # exclude saturated scales
  unique() %>%
  filter_at(
    vars(c(
      cell_grouping, AOO, mean_area)), 
    any_vars(!is.na(.)))

## Variables:
OAR_list_sp <- list()
OAR_list_sp_tp <- list()
OAR_list_sp_tp_atlas <- list()

## Loop:
for (n_atlas in seq_along(atlas_names)){
  atlas_name <- atlas_names[n_atlas]
  atlas <- dd %>% filter(dataset == atlas_name)

  for (time in seq_along(time_periods)){
    period_nr <-  time_periods[time]
    atlas_1time <- atlas %>% filter(tp == period_nr) 
    sp <- unique(atlas_1time$verbatim_name)
    
    for(spec in seq_along(sp)){
      species <- sp[spec]
      model_df <- atlas_1time %>% 
        filter(verbatim_name == species) %>% 
        distinct()

      OAR <- lm(log(AOO) ~ log(mean_area), data = model_df)
      
      OAR_df <- data.frame(
        verbatim_name = species,
        dataset = atlas_name,
        tp = period_nr,
        m_AOO_a = OAR$coefficients[2],
        b_AOO_a = OAR$coefficients[1])
      
      OAR_df$D_AOO_a <- -2*OAR_df$m_AOO_a+2
      OAR_df$dataset <- as.factor(OAR_df$dataset)
      OAR_list_sp[[spec]] <- OAR_df
      
    }
    
    OAR_df_sp <- plyr::rbind.fill(OAR_list_sp, fill = T)
    OAR_list_sp_tp[[time]] <- OAR_df_sp
     
  }
  OAR_df_sp_tp <- plyr::rbind.fill(OAR_list_sp_tp, fill = T)   
  OAR_list_sp_tp_atlas[[n_atlas]] <- OAR_df_sp_tp
}
    
OAR_final <- plyr::rbind.fill(OAR_list_sp_tp_atlas, fill = T) %>% distinct()
colSums(is.na(OAR_final))


species_data_new <- merge(sp_data_new, 
                          OAR_final, 
                          by=c(intersect(names(sp_data_new), names(OAR_final))), all = T) %>% distinct()

### =============================================== ###
rm(dd, OAR_list_sp_tp_atlas, OAR_list_sp_tp, OAR_df_sp_tp, OAR_list_sp, OAR_df_sp, OAR_df, sp, atlas_1time, model_df, atlas_name, OAR_final, n_atlas, OAR, period_nr, spec, species, species_names, time )


colSums(is.na(species_data_new))

species_data_new %>% 
    filter(exclude == 1) %>%    
    group_by(dataset, tp) %>% 
    summarize(n = n_distinct(verbatim_name)) 
### =============================================== ###

```

## 3. Telfer index of relative change
```{r}
### Since I cannot  download the sparta package right now, I copied the source code of the functions below: 


telfer <- function(taxa, site, time_period, minSite = 5, useIterations = TRUE, iterations = 10){
  
  # Perform error checks
  errorChecks(taxa = taxa, site = site, time_period = time_period, minSite = minSite,
              useIterations = useIterations, iterations = iterations)
 
  taxa_data <- data.frame(taxa, site, time_period)
  
  # Create a list of all pairwise comparisons of time periods
  TP_combos <- t(combn(x = sort(unique(time_period)), m = 2))
  
  # For each pair of time periods go through and compare them
  TelferList <- apply(X = TP_combos, MARGIN = 1, FUN = function(TPs){
          
    # Do the core Telfer analysis
    basic_temp <- telfer_func(taxa_data[taxa_data$time_period %in% TPs,], iterations = iterations,
                              useIterations = useIterations, minSite = minSite)[[1]]
    
    colnames(basic_temp)[4] <- paste('Telfer_', TPs[1], '_', TPs[2], sep = '')
    colnames(basic_temp)[3] <- paste('Nsite_', TPs[2], sep = '')
    colnames(basic_temp)[2] <- paste('Nsite_', TPs[1], sep = '')
    
    #Add in NAs
    basic_temp <- merge(basic_temp, data.frame(taxa = sort(unique(taxa))), all = TRUE)
        
    return(basic_temp)
    
  })

  Telfer_out <- Reduce(function(a,b) merge(a, b, all = TRUE, by = "taxa"), TelferList)  
  
  return(Telfer_out)
  
}

```

```{r}
# Internal function that uses the implementation of Telfer written by Gary Powney

telfer_func <- function (taxa_data, iterations = 10, useIterations = TRUE, minSite = 5){
  
  # Check we have two time periods
  TPs <- sort(unique(taxa_data$time_period))
  if(length(unique(taxa_data$time_period)) != 2) stop('There are not two time periods in the data')
    
  # subset data to one object for each time perod
  T1 <- unique(taxa_data[taxa_data$time_period == TPs[1], c('taxa', 'site')])
  T2 <- unique(taxa_data[taxa_data$time_period == TPs[2], c('taxa', 'site')])
  
  # remove gridcells that are only found in one time period
  T1cells <- unique(T1$site)
  T2cells <- unique(T2$site)
  allGood <- T1cells[T1cells %in% T2cells]
  T1 <- T1[T1$site %in% allGood,]
  T2 <- T2[T2$site %in% allGood,]
  
  ### Identify the number of sites each taxa occupies in each time period. ###
  T1_range <- data.frame(taxa = names(table(T1$taxa)), T1Nsites = as.numeric(table(T1$taxa)))
  T2_range <- data.frame(taxa = names(table(T2$taxa)), T2Nsites = as.numeric(table(T2$taxa)))
  
  ### Remove taxa which have less than minSite grid cells in first period ###
  T1_range_good <- T1_range[T1_range$T1Nsites >= minSite,]
  if(nrow(T1_range_good) == 0) stop(paste('No taxa satisfy the minSite criteria when comparing time',
                                          'period', TPs[1], 'and', TPs[2]))
  
  ### Link the two tables by taxa ###
  spp_table <- merge(T1_range_good, T2_range, by = 'taxa')
  
  ### Identify the simple difference between the two time periods ###
  spp_table$range_change <- spp_table$T2Nsites - spp_table$T1Nsites 

  ### convert the grid cell number to proportion of total number of cells surveyed ###
  total.cells <- length(allGood)
  
  # To avoid the problems associated with 0 proportions they were calculated as (x + 0.5) / (n + 1).
  spp_table$T1_range_prop <- (spp_table$T1Nsites + 0.5) / (total.cells + 1)	
  spp_table$T2_range_prop <- (spp_table$T2Nsites + 0.5) / (total.cells + 1)
  
  
  spp_table$T1_logit_range <- log(spp_table$T1_range_prop / (1 - spp_table$T1_range_prop))  # logit transform the proportions
  spp_table$T2_logit_range <- log(spp_table$T2_range_prop / (1 - spp_table$T2_range_prop))	# logit transform the proportions
  
  ### To account for non constant variance we must do the following steps to create a variable to weight the final regression ###
  row.names(spp_table) <- spp_table$taxa # name rows helps make sense of the model output
  m1 <- lm(T2_logit_range ~ T1_logit_range, data = spp_table)  	# linear regression of two time periods

  if(!useIterations){
    
    spp_table$change_index <- rstandard(m1)
    final_output_table <- spp_table[,c(1:3,9)]
    return(list(final_output_table, spp_table))
    
  } else if(useIterations){
    
    spp_table$sq_residual_m1 <- resid(m1)^2 # square the residuals
    spp_table$fitted_proportions <- ilt(m1$fitted) # exponential function, overwrite fitted values to fitted proportions
    
    total.cell.1 <- total.cells + 1								# total grid cells surveyed + 1 
    
    spp_table$NP_test <- 1 / (total.cell.1 * spp_table$fitted_proportions * 
                                (1 - spp_table$fitted_proportions))	# 1 / [NP (1 - P)] in telfer paper
    
    # second model which is the squared residuals of m1 on 1/[NP (1-P)]
    m2 <- lm(sq_residual_m1 ~ NP_test, data = spp_table)
    
    # second model which is the squared residuals of m1 on 1/[NP (1-P)]
    c_ <- coef(m2)[1] # take the intercept of m2 
    d_ <- coef(m2)[2] # take the slope of m2
    
    V_ <- NULL												# prepare variance vector
    rep_loop <- 1:(iterations-1)		# repeat the process 
    C_all <- c_												# prepare c vector
    D_all <- d_												# prepare d vector
    V_all <- V_												# prepare variance vector
    
    
    for (i in rep_loop){  							
      
      # c_ + d_ should not be in brackets as they were in a previous version
      V_ <- c_ + d_ / (total.cell.1 * spp_table$fitted_proportions * (1 - spp_table$fitted_proportions))	# work out the variance (modification of the binomial proportion variance structure)
      
      # take the inverse of variance squared.
      inv_sq_V <- 1 / (V_^2)										
      
      # run the new model weighting by the inverse variance identifeid above.
      error <- try(m4 <- lm(sq_residual_m1 ~ NP_test, weights = inv_sq_V, data = spp_table), silent = TRUE)
      if(class(error) == 'try-error') stop('Model failed in iteration, too little data?')
      
      c_ <- coef(m4)[1] # take the intercept of the new model
      d_ <- coef(m4)[2] # take the slope of the new model
      
      C_all <- c(C_all,c_) # build a vector with all of the new intercepts
      D_all <- c(D_all,d_) # build a vector with all of the new slopes
      V_all <- c(V_all,V_) # build a vector with all of the new inverse variances squared
      
    }
  
    ## final model to take the residuals from
    # take the reciprocal of the "settled" variance (Telfer et al 2002)
    spp_table$recip_V <- 1 / V_  										
    # use the reciprocal of the "settled" variance as weight for the final model.
    m1 <- lm(T2_logit_range ~ T1_logit_range, weights = spp_table$recip_V, data = spp_table)
    # take the standardised residuals from the model.
    # For each taxa, the standardised residual from the fitted regression line provides
    # the index of relative change in range size. A taxa with a negative change index has
    # been recorded in relatively fewer grid cells in the later period, whereas a taxa with
    # a positive change index has been recorded in relatively more.  
    spp_table$change_index <- rstandard(m1)								
    final_output_table <- spp_table[,c(1:3,13)]	

    return(list(final_output_table, spp_table))
    
  }

}
```


```{r}
errorChecks <- function(taxa = NULL, site = NULL, survey = NULL, replicate = NULL, closure_period = NULL, time_period = NULL, 
                        startDate = NULL, endDate = NULL, Date = NULL, 
                        time_periodsDF = NULL, dist = NULL, sim = NULL,
                        dist_sub = NULL, sim_sub = NULL, minSite = NULL, useIterations = NULL,
                        iterations = NULL, overdispersion = NULL, verbose = NULL,
                        list_length = NULL, site_effect = NULL, family = NULL,
                        n_iterations = NULL, burnin = NULL, thinning = NULL,
                        n_chains = NULL, seed = NULL, year_col = NULL, site_col = NULL,
                        sp_col = NULL, start_col = NULL, end_col = NULL, phi = NULL,
                        alpha = NULL, non_benchmark_sp = NULL, fres_site_filter = NULL,
                        time_periods = NULL, frespath = NULL, species_to_include = NULL){
  
  # Create a list of all non-null arguements that should be of equal length
  valid_argumentsTEMP <- list(taxa=taxa,
                          site=site,
                          survey=survey,
                          closure_period=closure_period,
                          replicate=replicate,
                          time_period=time_period,
                          startDate=startDate,
                          endDate=endDate)
  valid_arguments <- valid_argumentsTEMP[!unlist(lapply(valid_argumentsTEMP, FUN = is.null))]
  
  # Check these are all the same length
  if(length(valid_arguments) > 0){
    lengths <- sapply(valid_arguments, length)
    # This tests if all are the same
    if(abs(max(lengths) - min(lengths)) > .Machine$double.eps ^ 0.5){
      stop(paste('The following arguements are not of equal length:', paste(names(valid_arguments), collapse = ', ')))
    }
  }
  
  if(!is.null(taxa) & !is.null(site) & !is.null(survey)){

    if(!is.null(replicate)){
      df <- data.frame(taxa, site, survey, replicate)
    } else {
      df <- data.frame(taxa, site, survey)
    }

    NR1 <- nrow(df)
    NR2 <- nrow(distinct(df))
    
    if(NR1 != NR2) warning(paste(NR1 - NR2, 'out of', NR1, 'observations will be removed as duplicates'))
    
  }
  
  if(!is.null(taxa) & !is.null(site) & !is.null(time_period)){
    
    df <- data.frame(taxa, site, time_period)
    NR1 <- nrow(df)
    NR2 <- nrow(distinct(df))
    
    if(NR1 != NR2) warning(paste(NR1 - NR2, 'out of', NR1, 'observations will be removed as duplicates'))
    
  }
  
  ###### Make sure there are no NAs
  
  ### Checks for taxa ###
  if(!is.null(taxa)){    
    if(!all(!is.na(taxa))) stop('taxa must not contain NAs')    
  }
  
  ### Checks for site ###
  if(!is.null(site)){    
    if(!all(!is.na(site))) stop('site must not contain NAs')
    if(!all(site != '')) stop("site must not contain empty values (i.e. '')")
  }
  
  ### Checks for closure period ###
  if(!is.null(closure_period)){    
    if(!all(!is.na(closure_period))) stop('closure_period must not contain NAs')    
  }
  
  ### Checks for replicate ###
  if(!is.null(replicate)){    
    if(!all(!is.na(replicate))) stop('replicate must not contain NAs')    
  }
  
  ### Checks for time_period ###
  if(!is.null(time_period)){    
    if(!all(!is.na(time_period))) stop('time_period must not contain NAs')    
  }
  
  ### Checks for startDate ###
  if(!is.null(startDate)){
    if(!'POSIXct' %in% class(startDate) & !'Date' %in% class(startDate)){
      stop(paste('startDate is not in a date format. This should be of class "Date" or "POSIXct"'))
    }
    # Make sure there are no NAs
    if(!all(!is.na(startDate))) stop('startDate must not contain NAs')
  }
  
  ### Checks for Date ###
  if(!is.null(Date)){
    if(!'POSIXct' %in% class(Date) & !'Date' %in% class(Date) & !'data.frame' %in% class(Date)){
      stop(paste('Date must be a data.frame or date vector'))
    }
    # Make sure there are no NAs
    if(!all(!is.na(Date))) stop('Date must not contain NAs')
  }
  
  ### Checks for endDate ###
  if(!is.null(endDate)){
    if(!'POSIXct' %in% class(endDate) & !'Date' %in% class(endDate)){
      stop(paste('endDate is not in a date format. This should be of class "Date" or "POSIXct"'))
    }
    # Make sure there are no NAs
    if(!all(!is.na(endDate))) stop('endDate must not contain NAs')
  }
  
  ### Checks for time_periodsDF ###
  if(!is.null(time_periodsDF)){
    # Ensure end year is after start year
    if(any(time_periodsDF[,2] < time_periodsDF[,1])) stop('In time_periods end years must be greater than or equal to start years')
    
    # Ensure year ranges don't overlap
    starts <- tail(time_periodsDF$start, -1)
    ends <- head(time_periodsDF$end, -1)
    if(any(ends > starts)) stop('In time_periods year ranges cannot overlap')  
  }
  
  ### Checks for dist ###
  if(!is.null(dist)){
    
    if(class(dist) != 'data.frame') stop('dist must be a data.frame')
    if(ncol(dist) != 3) stop('dist must have three columns') 
    if(!class(dist[,3]) %in% c('numeric', 'integer')) stop('the value column in dist must be an integer or numeric')
    
    # Check distance table contains all combinations of sites
    sites <- unique(c(as.character(dist[,1]), as.character(dist[,2])))
    combinations_temp <- merge(sites, sites)
    all_combinations <- paste(combinations_temp[,1],combinations_temp[,2])
    data_combinations <- paste(dist[,1],dist[,2])
    if(!all(all_combinations %in% data_combinations)){
      stop('dist table does not include all possible combinations of sites')
    }    
  }
  
  ### Checks for sim ###
  if(!is.null(sim)){
    
    if(class(sim) != 'data.frame') stop('sim must be a data.frame')
    if(!all(lapply(sim[,2:ncol(sim)], class) %in% c('numeric', 'integer'))) stop('the values in sim must be integers or numeric')
        
  }
  
  ### Checks for sim_sub and dist_sub ###
  if(!is.null(sim_sub) & !is.null(dist_sub)){
    
    if(!class(dist_sub) %in% c('numeric', 'integer')) stop('dist_sub must be integer or numeric')
    if(!class(sim_sub) %in% c('numeric', 'integer')) stop('sim_sub must be integer or numeric')
    if(dist_sub <= sim_sub) stop("'dist_sub' cannot be smaller than or equal to 'sim_sub'")
    
  }
  
  ### checks for minSite ###
  if(!is.null(minSite)){
  
    if(!class(minSite) %in% c('numeric', 'integer')) stop('minSite must be numeric or integer')
  
  }
  
  ### checks for useIterations ###
  if(!is.null(useIterations)){
    
    if(class(useIterations) != 'logical') stop('useIterations must be logical')
  
  }
  
  ### checks for iterations ###
  if(!is.null(iterations)){
   
    if(!class(iterations) %in% c('numeric', 'integer')) stop('iterations must be numeric or integer')
        
  }
  
  ### checks for overdispersion ###
  if(!is.null(overdispersion)){
    
    if(class(overdispersion) != 'logical') stop('overdispersion must be logical')
    
  }
  
  ### checks for verbose ###
  if(!is.null(verbose)){
    
    if(class(verbose) != 'logical') stop('verbose must be logical')
    
  }
  
  ### checks for list_length ###
  if(!is.null(list_length)){
    
    if(class(list_length) != 'logical') stop('list_length must be logical')
    
  }
  
  ### checks for site_effect ###
  if(!is.null(site_effect)){
    
    if(class(site_effect) != 'logical') stop('site_effect must be logical')
    
  }  
  
  ### checks for family ###
  if(!is.null(family)){
    
    if(!family %in% c('Binomial', 'Bernoulli')){
      
      stop('family must be either Binomial or Bernoulli')
      
    }
    
    if(!is.null(list_length)){
      
      if(list_length & family == 'Binomial'){
        warning('When list_length is TRUE family will default to Bernoulli')
      }      
    }
  }
  
  ### checks for species_to_include ###
  
  if(!is.null(species_to_include)){
    
    missing_species <- species_to_include[!species_to_include %in% unique(taxa)]
    
    if(length(missing_species) > 0){
      
      warning('The following species in species_to_include are not in your data: ',
           paste(missing_species, collapse = ', '))
      
    }
  }
  
  ### check BUGS parameters ###
  if(!is.null(c(n_iterations, burnin, thinning, n_chains))){
    if(!is.numeric(n_iterations)) stop('n_iterations should be numeric')
    if(!is.numeric(burnin)) stop('burnin should be numeric')
    if(!is.numeric(thinning)) stop('thinning should be numeric')
    if(!is.numeric(n_chains)) stop('n_chains should be numeric')
    
    
    if(burnin > n_iterations) stop('Burn in (burnin) must not be larger that the number of iteration (n_iterations)')
    if(thinning > n_iterations) stop('thinning must not be larger that the number of iteration (n_iterations)')
    
  }
  
  if(!is.null(seed)){
    
    if(!is.numeric(seed)) stop('seed muct be numeric')
    
  }  
  
  ## Checks for frescalo
  if(!is.null(year_col)){
    if(is.na(year_col)){
      if(!is.null(start_col) & !is.null(end_col)){
        if(is.na(start_col)|is.na(end_col)){
          stop('year_col or start_col and end_col must be given')
        } else {  
          if(!is.na(start_col)|!is.na(end_col)){
            stop('year_col cannot be used at the same time as start_col and end_col')
          }
        }
      }
    }
  }
  
  if(!is.null(phi)){
    if(phi>0.95|phi<0.5){
      stop("phi is outside permitted range of 0.50 to 0.95")
    } 
  }
  
  if(!is.null(alpha)){
    if(alpha>0.5|alpha<0.08){
      stop("alpha is outside permitted range of 0.08 to 0.50")
    } 
  }
  
  if(!is.null(non_benchmark_sp)){    
    if(any(!is.vector(non_benchmark_sp), !is.character(non_benchmark_sp))){
        stop('non_benchmark_sp must be a character vector')
    }
  }
  
  if(!is.null(fres_site_filter)){
    if(any(!is.vector(fres_site_filter), !is.character(fres_site_filter))){
      stop('fres_site_filter must be a character vector')
    }  
  }
  
  if(!is.null(time_periods)){
    if(!is.data.frame(time_periods)) stop('time_periods should be a data.frame. e.g. "data.frame(start=c(1980,1990),end=c(1989,1999))"')
  }
  
  if(!is.null(frespath)){
    if(!grepl('.exe$', tolower(frespath))) stop("filepath is not the path to a '.exe' file") 
    if(!file.exists(frespath)) stop(paste(frespath, 'does not exist'))
  }
}









ilt <-function(x) exp(x)/(1+exp(x))
```


```{r}
# Telfer:
telfer_res <- list()
for (i in seq_along(atlas_names)){
  df <- occ_data_final %>% 
    filter(cell_grouping == 1 & dataset == atlas_names[i]) %>%
    distinct(dataset, tp, verbatim_name, cell_label) 
  telfer_df <- telfer(taxa = df$verbatim_name,
                         site = df$cell_label,
                         time_period = df$tp,
                         minSite = 1)
telfer_df$dataset <- atlas_names[i]
telfer_res[[i]] <- telfer_df
  
}

telfer_res_df <- plyr::rbind.fill(telfer_res, fill = T) %>% 
  distinct() %>%
  select(taxa, Telfer_1_2, dataset) %>% 
  rename(verbatim_name = taxa)%>%
  reorder_levels(dataset, order = atlas_names)

telfer_res_df$Telfer_1_2 <- round(telfer_res_df$Telfer_1_2, 3)
species_data_new2 <- left_join(species_data_new, telfer_res_df)

### =============================================== ###
colSums(is.na(species_data_new2))
rm(atlas, df, species_data_new, sp_data_new,telfer, telfer_res_df, i, telfer_res )
### =============================================== ###
```

## 4. Log Ratio of Change in Occupancy
### 4.1 Long to wide format

```{r}

wide_dfs <- list()
for (i in seq_along(time_periods)){

    wide_dfs[[i]] <- species_data_new2 %>%
        filter(cell_grouping == 1) %>% 
        distinct(dataset, tp, verbatim_name, AOO) %>%
        group_by(dataset, tp, verbatim_name) %>%
        filter(tp == time_periods[i]) %>%
        setNames(paste0('tp', i, '_', names(.))) %>% 
        ungroup() %>%
        select(-c(paste0("tp", i, "_tp"))) %>%
        rename(
            verbatim_name = paste0("tp", i, "_verbatim_name"),
            dataset = paste0("tp", i, "_dataset"))
       }

sp_dat_wide <- merge(wide_dfs[[1]], wide_dfs[[2]])
colSums(is.na(sp_dat_wide))


### =============================================== ###
rm(wide_dfs, i)
### =============================================== ###
```

## 4.2 Calculate Log Ratio of Occupancy Change


```{r}
# Calculate log-Ratio of AOO (Temporal change)
logRatio <- sp_dat_wide %>% 
  mutate(log_R2_1 = log(tp2_AOO/tp1_AOO)) %>%
  select(-tp1_AOO, -tp2_AOO)

big_table <- full_join(species_data_new2, logRatio) %>% filter(cell_grouping == 1) %>%
    distinct(dataset, tp, cell_grouping, verbatim_name, .keep_all=T) %>%
    mutate_if(is.numeric, round, 3)


big_table %>% group_by(dataset, tp) %>% 
  summarise(across(everything(), ~ sum(is.na(.x))))

### =============================================== ###
rm(logRatio,sp_dat_wide)
### =============================================== ###

```


# Save Output:
```{r}
saveRDS(occ_data_final, "../../out/rds/SpeciesDatPerCell.rds") # data per cell
saveRDS(big_table, "../../out/predictors/BigTable.rds") # data per species
save.image("../../out/RData/2_Atlas_Var_Calc.RData")

```